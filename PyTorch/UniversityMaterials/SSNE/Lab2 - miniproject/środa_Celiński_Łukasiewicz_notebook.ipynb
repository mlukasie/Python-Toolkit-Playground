{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-12T15:50:20.407389Z",
     "start_time": "2025-03-12T15:50:20.401622Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:44:50.352410Z",
     "start_time": "2025-03-12T15:44:50.310239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_raw = pd.read_csv('data.csv')\n",
    "df_raw.head()\n",
    "\n",
    "df = df_raw.copy()\n",
    "df.head()"
   ],
   "id": "cbc6e671bcb28566",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:44:52.612690Z",
     "start_time": "2025-03-12T15:44:52.592051Z"
    }
   },
   "cell_type": "code",
   "source": "df.info()",
   "id": "ab3974e576056b22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10886 entries, 0 to 10885\n",
      "Data columns (total 17 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   instant     10886 non-null  int64  \n",
      " 1   dteday      10886 non-null  object \n",
      " 2   season      10886 non-null  int64  \n",
      " 3   yr          10886 non-null  int64  \n",
      " 4   mnth        10886 non-null  int64  \n",
      " 5   hr          10886 non-null  int64  \n",
      " 6   holiday     10886 non-null  int64  \n",
      " 7   weekday     10886 non-null  int64  \n",
      " 8   workingday  10886 non-null  int64  \n",
      " 9   weathersit  10886 non-null  int64  \n",
      " 10  temp        10886 non-null  float64\n",
      " 11  atemp       10886 non-null  float64\n",
      " 12  hum         10886 non-null  float64\n",
      " 13  windspeed   10886 non-null  float64\n",
      " 14  casual      10886 non-null  int64  \n",
      " 15  registered  10886 non-null  int64  \n",
      " 16  cnt         10886 non-null  int64  \n",
      "dtypes: float64(4), int64(12), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T15:45:41.824829Z",
     "start_time": "2025-03-12T15:45:41.804902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.drop(['instant', 'dteday'], axis=1, inplace=True)\n",
    "df.info()\n",
    "\n"
   ],
   "id": "d711e25a10d376c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10886 entries, 0 to 10885\n",
      "Data columns (total 15 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   season      10886 non-null  int64  \n",
      " 1   yr          10886 non-null  int64  \n",
      " 2   mnth        10886 non-null  int64  \n",
      " 3   hr          10886 non-null  int64  \n",
      " 4   holiday     10886 non-null  int64  \n",
      " 5   weekday     10886 non-null  int64  \n",
      " 6   workingday  10886 non-null  int64  \n",
      " 7   weathersit  10886 non-null  int64  \n",
      " 8   temp        10886 non-null  float64\n",
      " 9   atemp       10886 non-null  float64\n",
      " 10  hum         10886 non-null  float64\n",
      " 11  windspeed   10886 non-null  float64\n",
      " 12  casual      10886 non-null  int64  \n",
      " 13  registered  10886 non-null  int64  \n",
      " 14  cnt         10886 non-null  int64  \n",
      "dtypes: float64(4), int64(11)\n",
      "memory usage: 1.2 MB\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:05:15.237968Z",
     "start_time": "2025-03-12T16:05:15.225704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ScooterClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
    "        super().__init__()\n",
    "        # Initialize the modules we need to build the network\n",
    "        self.linear1 = nn.Linear(num_inputs, num_hidden, dtype=torch.float64)\n",
    "        self.act_fn = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(num_hidden, num_hidden, dtype=torch.float64)\n",
    "        self.linear3 = nn.Linear(num_hidden, num_hidden, dtype=torch.float64)\n",
    "        self.linear4 = nn.Linear(num_hidden, num_outputs, dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform the calculation of the model to determine the prediction\n",
    "        x = self.linear1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.linear4(x)\n",
    "        return x"
   ],
   "id": "d956e494844e5f64",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:05:17.561962Z",
     "start_time": "2025-03-12T16:05:17.546245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train=df.sample(frac=0.8,random_state=200) #random state is a seed value\n",
    "test=df.drop(train.index)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=200)\n",
    "print(f\"Train size: {train.shape}\")\n",
    "print(f\"Validation size: {val.shape}\")\n",
    "print(f\"Test size: {test.shape}\")"
   ],
   "id": "d694c6dd49f66045",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (6967, 15)\n",
      "Validation size: (1742, 15)\n",
      "Test size: (2177, 15)\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:24:42.901965Z",
     "start_time": "2025-03-12T16:24:42.894590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = train.values[:,:-3]\n",
    "X_test = torch.from_numpy(test.values[:,:-3]).cuda()\n",
    "X_val = torch.from_numpy(val.values[:,:-3]).cuda()\n",
    "\n",
    "y_val = torch.from_numpy(val.values[:,-1]).cuda()\n",
    "y_test = torch.from_numpy(test.values[:,-1]).cuda()\n",
    "\n",
    "train_dataset = data.TensorDataset(torch.from_numpy(X_train),torch.from_numpy(train.values[:,-1]))"
   ],
   "id": "ed538c4d700714e0",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:24:45.494977Z",
     "start_time": "2025-03-12T16:24:45.476680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = ScooterClassifier(num_inputs=12, num_hidden=512, num_outputs=1).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_module = nn.MSELoss(reduction=\"mean\")\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n"
   ],
   "id": "13d037d3526c1d5f",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:25:10.109643Z",
     "start_time": "2025-03-12T16:25:10.098763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X)\n",
    "        preds = preds.squeeze(dim=1)\n",
    "\n",
    "        loss = F.mse_loss(preds, y)\n",
    "\n",
    "    model.train()\n",
    "    return loss"
   ],
   "id": "fafa52ff576a454f",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:27:10.028512Z",
     "start_time": "2025-03-12T16:25:49.028420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "model.train()\n",
    "device = torch.device(\"cuda\")\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    true_preds, num_preds = 0., 0.\n",
    "    for data_inputs, data_labels in train_data_loader:\n",
    "        ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
    "        data_inputs = data_inputs.to(device, dtype=torch.float64)\n",
    "        data_labels = data_labels.to(device)\n",
    "\n",
    "        ## Step 2: Run the model on the input data\n",
    "        preds = model(data_inputs)\n",
    "        preds = preds.squeeze(dim=1)\n",
    "\n",
    "        ## Step 3: Calculate the loss\n",
    "        loss = loss_module(preds, data_labels)\n",
    "\n",
    "        ## Step 4: Perform backpropagation\n",
    "        # Before calculating the gradients, we need to ensure that they are all zero.\n",
    "        # The gradients would not be overwritten, but actually added to the existing ones.\n",
    "        optimizer.zero_grad()\n",
    "        # Perform backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        ## Step 5: Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch}, train_loss: {np.sqrt(loss.item()):.3}, vsl_loss: {np.sqrt(evaluate(model, X_val, y_val).item())}\")\n",
    "\n"
   ],
   "id": "b18dd381329cce8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train_loss: 83.7, vsl_loss: 103.75082916442803\n",
      "Epoch: 1, train_loss: 1.03e+02, vsl_loss: 102.82872242878973\n",
      "Epoch: 2, train_loss: 1.01e+02, vsl_loss: 101.35360815818244\n",
      "Epoch: 3, train_loss: 93.7, vsl_loss: 106.55641969793862\n",
      "Epoch: 4, train_loss: 1.05e+02, vsl_loss: 100.76040260829899\n",
      "Epoch: 5, train_loss: 93.1, vsl_loss: 98.31826888655661\n",
      "Epoch: 6, train_loss: 1.12e+02, vsl_loss: 96.61308318797191\n",
      "Epoch: 7, train_loss: 1.09e+02, vsl_loss: 96.5018038342987\n",
      "Epoch: 8, train_loss: 80.6, vsl_loss: 96.09811100396198\n",
      "Epoch: 9, train_loss: 94.3, vsl_loss: 108.05760574574929\n",
      "Epoch: 10, train_loss: 83.2, vsl_loss: 97.01442649008743\n",
      "Epoch: 11, train_loss: 94.5, vsl_loss: 96.15328570672935\n",
      "Epoch: 12, train_loss: 80.0, vsl_loss: 91.667338605255\n",
      "Epoch: 13, train_loss: 75.1, vsl_loss: 91.64549351288953\n",
      "Epoch: 14, train_loss: 1.13e+02, vsl_loss: 88.86254693985204\n",
      "Epoch: 15, train_loss: 66.3, vsl_loss: 102.37405463076982\n",
      "Epoch: 16, train_loss: 79.6, vsl_loss: 88.07638621432623\n",
      "Epoch: 17, train_loss: 57.7, vsl_loss: 86.59808802267722\n",
      "Epoch: 18, train_loss: 74.4, vsl_loss: 85.73923094506061\n",
      "Epoch: 19, train_loss: 87.6, vsl_loss: 87.52674572365181\n",
      "Epoch: 20, train_loss: 61.0, vsl_loss: 96.75839243115419\n",
      "Epoch: 21, train_loss: 68.2, vsl_loss: 91.62182685898085\n",
      "Epoch: 22, train_loss: 77.7, vsl_loss: 84.27888369088298\n",
      "Epoch: 23, train_loss: 75.9, vsl_loss: 85.7698310194657\n",
      "Epoch: 24, train_loss: 77.6, vsl_loss: 86.79017063885033\n",
      "Epoch: 25, train_loss: 1.07e+02, vsl_loss: 98.55251105673202\n",
      "Epoch: 26, train_loss: 78.9, vsl_loss: 83.3309540907142\n",
      "Epoch: 27, train_loss: 73.8, vsl_loss: 80.7548217451505\n",
      "Epoch: 28, train_loss: 1e+02, vsl_loss: 80.87439658579615\n",
      "Epoch: 29, train_loss: 60.0, vsl_loss: 83.46826242974159\n",
      "Epoch: 30, train_loss: 73.7, vsl_loss: 80.4468348366195\n",
      "Epoch: 31, train_loss: 76.9, vsl_loss: 82.43502353713677\n",
      "Epoch: 32, train_loss: 73.6, vsl_loss: 78.9959207937755\n",
      "Epoch: 33, train_loss: 71.9, vsl_loss: 77.95141842167529\n",
      "Epoch: 34, train_loss: 53.5, vsl_loss: 78.39377349182602\n",
      "Epoch: 35, train_loss: 69.8, vsl_loss: 76.85057341762622\n",
      "Epoch: 36, train_loss: 58.0, vsl_loss: 77.8725088862399\n",
      "Epoch: 37, train_loss: 61.5, vsl_loss: 75.48698800643267\n",
      "Epoch: 38, train_loss: 76.4, vsl_loss: 77.36747185392893\n",
      "Epoch: 39, train_loss: 71.8, vsl_loss: 76.38695997430632\n",
      "Epoch: 40, train_loss: 70.8, vsl_loss: 76.13952897388732\n",
      "Epoch: 41, train_loss: 65.9, vsl_loss: 77.98618536829198\n",
      "Epoch: 42, train_loss: 46.0, vsl_loss: 80.33921436603983\n",
      "Epoch: 43, train_loss: 69.7, vsl_loss: 73.64362599281334\n",
      "Epoch: 44, train_loss: 56.9, vsl_loss: 81.64970786105694\n",
      "Epoch: 45, train_loss: 83.5, vsl_loss: 80.94720552895205\n",
      "Epoch: 46, train_loss: 81.9, vsl_loss: 81.3330278829557\n",
      "Epoch: 47, train_loss: 64.4, vsl_loss: 77.46876052436019\n",
      "Epoch: 48, train_loss: 66.2, vsl_loss: 72.32905136956187\n",
      "Epoch: 49, train_loss: 74.9, vsl_loss: 72.29526125072559\n",
      "Epoch: 50, train_loss: 74.7, vsl_loss: 71.6216279563879\n",
      "Epoch: 51, train_loss: 89.3, vsl_loss: 95.7393572164811\n",
      "Epoch: 52, train_loss: 78.2, vsl_loss: 81.88956998204256\n",
      "Epoch: 53, train_loss: 65.9, vsl_loss: 76.79456724671071\n",
      "Epoch: 54, train_loss: 69.0, vsl_loss: 75.55346512605932\n",
      "Epoch: 55, train_loss: 61.0, vsl_loss: 75.50331352960511\n",
      "Epoch: 56, train_loss: 62.0, vsl_loss: 69.62872015064553\n",
      "Epoch: 57, train_loss: 73.9, vsl_loss: 70.05585062325233\n",
      "Epoch: 58, train_loss: 50.0, vsl_loss: 70.04147543255968\n",
      "Epoch: 59, train_loss: 41.8, vsl_loss: 72.52519798484829\n",
      "Epoch: 60, train_loss: 53.5, vsl_loss: 74.47768991645769\n",
      "Epoch: 61, train_loss: 66.8, vsl_loss: 72.77649253120134\n",
      "Epoch: 62, train_loss: 75.1, vsl_loss: 68.65988690039593\n",
      "Epoch: 63, train_loss: 57.3, vsl_loss: 72.57320965358868\n",
      "Epoch: 64, train_loss: 42.6, vsl_loss: 68.63604597805904\n",
      "Epoch: 65, train_loss: 59.5, vsl_loss: 69.27634720746808\n",
      "Epoch: 66, train_loss: 54.2, vsl_loss: 68.45955193742428\n",
      "Epoch: 67, train_loss: 66.4, vsl_loss: 68.59428292866824\n",
      "Epoch: 68, train_loss: 65.9, vsl_loss: 68.14487522583636\n",
      "Epoch: 69, train_loss: 50.2, vsl_loss: 66.63607944502279\n",
      "Epoch: 70, train_loss: 73.4, vsl_loss: 69.79150469494557\n",
      "Epoch: 71, train_loss: 62.5, vsl_loss: 67.63860592881441\n",
      "Epoch: 72, train_loss: 69.6, vsl_loss: 68.71572464869587\n",
      "Epoch: 73, train_loss: 75.8, vsl_loss: 67.80571344727343\n",
      "Epoch: 74, train_loss: 42.6, vsl_loss: 68.22688706895002\n",
      "Epoch: 75, train_loss: 46.9, vsl_loss: 77.51878413212272\n",
      "Epoch: 76, train_loss: 63.9, vsl_loss: 65.32977892570817\n",
      "Epoch: 77, train_loss: 65.5, vsl_loss: 65.45248117201943\n",
      "Epoch: 78, train_loss: 43.7, vsl_loss: 64.9677352205589\n",
      "Epoch: 79, train_loss: 83.0, vsl_loss: 67.60895448132239\n",
      "Epoch: 80, train_loss: 56.7, vsl_loss: 63.31916317928653\n",
      "Epoch: 81, train_loss: 41.7, vsl_loss: 64.53453342201966\n",
      "Epoch: 82, train_loss: 66.8, vsl_loss: 63.40438313156209\n",
      "Epoch: 83, train_loss: 69.1, vsl_loss: 63.5644215788491\n",
      "Epoch: 84, train_loss: 52.4, vsl_loss: 63.98996291669022\n",
      "Epoch: 85, train_loss: 56.3, vsl_loss: 62.56350166389615\n",
      "Epoch: 86, train_loss: 60.0, vsl_loss: 63.57349100122424\n",
      "Epoch: 87, train_loss: 60.0, vsl_loss: 63.82921231235848\n",
      "Epoch: 88, train_loss: 54.1, vsl_loss: 63.84868608499282\n",
      "Epoch: 89, train_loss: 56.2, vsl_loss: 64.18233183452712\n",
      "Epoch: 90, train_loss: 57.1, vsl_loss: 71.3600968854462\n",
      "Epoch: 91, train_loss: 50.8, vsl_loss: 62.298685328698525\n",
      "Epoch: 92, train_loss: 58.9, vsl_loss: 69.70794365036485\n",
      "Epoch: 93, train_loss: 77.2, vsl_loss: 64.25681686515435\n",
      "Epoch: 94, train_loss: 44.1, vsl_loss: 68.39626650358777\n",
      "Epoch: 95, train_loss: 56.5, vsl_loss: 61.41193298389391\n",
      "Epoch: 96, train_loss: 40.7, vsl_loss: 63.13758648509659\n",
      "Epoch: 97, train_loss: 61.6, vsl_loss: 75.09696753160608\n",
      "Epoch: 98, train_loss: 56.8, vsl_loss: 63.59059600026177\n",
      "Epoch: 99, train_loss: 66.3, vsl_loss: 83.56228123818204\n",
      "Epoch: 100, train_loss: 48.3, vsl_loss: 64.86200931717411\n",
      "Epoch: 101, train_loss: 68.8, vsl_loss: 61.35777881777204\n",
      "Epoch: 102, train_loss: 52.9, vsl_loss: 61.78400699852733\n",
      "Epoch: 103, train_loss: 65.9, vsl_loss: 60.78200549570636\n",
      "Epoch: 104, train_loss: 48.7, vsl_loss: 64.70985665463466\n",
      "Epoch: 105, train_loss: 55.2, vsl_loss: 88.97709666333768\n",
      "Epoch: 106, train_loss: 74.2, vsl_loss: 60.77041572277001\n",
      "Epoch: 107, train_loss: 38.0, vsl_loss: 61.09820350483621\n",
      "Epoch: 108, train_loss: 45.1, vsl_loss: 59.963358991344386\n",
      "Epoch: 109, train_loss: 67.2, vsl_loss: 58.998461746707086\n",
      "Epoch: 110, train_loss: 56.6, vsl_loss: 60.07555038942926\n",
      "Epoch: 111, train_loss: 54.9, vsl_loss: 61.84685125307503\n",
      "Epoch: 112, train_loss: 74.2, vsl_loss: 61.281495240952246\n",
      "Epoch: 113, train_loss: 44.5, vsl_loss: 60.36973253618415\n",
      "Epoch: 114, train_loss: 52.2, vsl_loss: 60.46210958380441\n",
      "Epoch: 115, train_loss: 36.4, vsl_loss: 58.2899477123463\n",
      "Epoch: 116, train_loss: 59.6, vsl_loss: 57.80531287919909\n",
      "Epoch: 117, train_loss: 65.1, vsl_loss: 61.73219216756483\n",
      "Epoch: 118, train_loss: 56.7, vsl_loss: 61.904572336571846\n",
      "Epoch: 119, train_loss: 53.3, vsl_loss: 58.41230772216\n",
      "Epoch: 120, train_loss: 50.9, vsl_loss: 57.13667525993394\n",
      "Epoch: 121, train_loss: 43.1, vsl_loss: 58.039547709277194\n",
      "Epoch: 122, train_loss: 53.1, vsl_loss: 64.74120994071704\n",
      "Epoch: 123, train_loss: 44.1, vsl_loss: 68.33651500387468\n",
      "Epoch: 124, train_loss: 44.7, vsl_loss: 65.4887429345578\n",
      "Epoch: 125, train_loss: 45.8, vsl_loss: 61.28993895942376\n",
      "Epoch: 126, train_loss: 57.5, vsl_loss: 58.75053581425924\n",
      "Epoch: 127, train_loss: 72.3, vsl_loss: 57.784869087418485\n",
      "Epoch: 128, train_loss: 57.1, vsl_loss: 59.59963710403312\n",
      "Epoch: 129, train_loss: 64.6, vsl_loss: 71.17045363451132\n",
      "Epoch: 130, train_loss: 54.4, vsl_loss: 57.4532541897923\n",
      "Epoch: 131, train_loss: 45.8, vsl_loss: 58.01351867922742\n",
      "Epoch: 132, train_loss: 42.1, vsl_loss: 56.595078407478105\n",
      "Epoch: 133, train_loss: 55.9, vsl_loss: 57.66886399817183\n",
      "Epoch: 134, train_loss: 41.1, vsl_loss: 56.453643158856416\n",
      "Epoch: 135, train_loss: 40.1, vsl_loss: 57.028476664229174\n",
      "Epoch: 136, train_loss: 53.7, vsl_loss: 56.948888331304055\n",
      "Epoch: 137, train_loss: 44.0, vsl_loss: 57.39095398096573\n",
      "Epoch: 138, train_loss: 36.7, vsl_loss: 55.216412713020404\n",
      "Epoch: 139, train_loss: 52.9, vsl_loss: 56.727786531246174\n",
      "Epoch: 140, train_loss: 55.9, vsl_loss: 61.187679502550736\n",
      "Epoch: 141, train_loss: 43.2, vsl_loss: 57.7521279887921\n",
      "Epoch: 142, train_loss: 41.4, vsl_loss: 59.678354140493234\n",
      "Epoch: 143, train_loss: 32.8, vsl_loss: 55.58273089287389\n",
      "Epoch: 144, train_loss: 47.5, vsl_loss: 55.73838480353989\n",
      "Epoch: 145, train_loss: 65.3, vsl_loss: 56.84543992085089\n",
      "Epoch: 146, train_loss: 45.5, vsl_loss: 59.82856991857657\n",
      "Epoch: 147, train_loss: 35.5, vsl_loss: 63.433635056291834\n",
      "Epoch: 148, train_loss: 27.7, vsl_loss: 56.34458766124508\n",
      "Epoch: 149, train_loss: 41.4, vsl_loss: 57.785630941008044\n",
      "Epoch: 150, train_loss: 65.7, vsl_loss: 65.1337296550512\n",
      "Epoch: 151, train_loss: 50.2, vsl_loss: 57.45634360110813\n",
      "Epoch: 152, train_loss: 42.4, vsl_loss: 58.186312967924515\n",
      "Epoch: 153, train_loss: 38.9, vsl_loss: 56.8099472678858\n",
      "Epoch: 154, train_loss: 43.9, vsl_loss: 53.95670239805847\n",
      "Epoch: 155, train_loss: 46.4, vsl_loss: 53.47958890189434\n",
      "Epoch: 156, train_loss: 53.5, vsl_loss: 54.41426387339533\n",
      "Epoch: 157, train_loss: 41.9, vsl_loss: 62.39049474359858\n",
      "Epoch: 158, train_loss: 28.1, vsl_loss: 55.61622275670026\n",
      "Epoch: 159, train_loss: 55.6, vsl_loss: 53.45621962120326\n",
      "Epoch: 160, train_loss: 33.5, vsl_loss: 59.16229896151757\n",
      "Epoch: 161, train_loss: 34.4, vsl_loss: 55.5600056591023\n",
      "Epoch: 162, train_loss: 49.3, vsl_loss: 67.12837693465619\n",
      "Epoch: 163, train_loss: 34.5, vsl_loss: 57.322760132341145\n",
      "Epoch: 164, train_loss: 40.6, vsl_loss: 66.27925811217366\n",
      "Epoch: 165, train_loss: 47.8, vsl_loss: 54.91851315039369\n",
      "Epoch: 166, train_loss: 38.4, vsl_loss: 54.69421055542426\n",
      "Epoch: 167, train_loss: 47.9, vsl_loss: 56.19979602593288\n",
      "Epoch: 168, train_loss: 50.8, vsl_loss: 54.25372212300109\n",
      "Epoch: 169, train_loss: 46.4, vsl_loss: 58.707123736803716\n",
      "Epoch: 170, train_loss: 35.1, vsl_loss: 55.154256420260374\n",
      "Epoch: 171, train_loss: 35.0, vsl_loss: 55.07039179690607\n",
      "Epoch: 172, train_loss: 51.9, vsl_loss: 57.09087832620624\n",
      "Epoch: 173, train_loss: 42.2, vsl_loss: 65.25183576788721\n",
      "Epoch: 174, train_loss: 48.3, vsl_loss: 56.78656700031015\n",
      "Epoch: 175, train_loss: 34.9, vsl_loss: 62.40858875525639\n",
      "Epoch: 176, train_loss: 42.0, vsl_loss: 55.00286196848261\n",
      "Epoch: 177, train_loss: 46.2, vsl_loss: 57.119330864317234\n",
      "Epoch: 178, train_loss: 39.8, vsl_loss: 52.320389991682966\n",
      "Epoch: 179, train_loss: 27.8, vsl_loss: 56.366640880067045\n",
      "Epoch: 180, train_loss: 53.4, vsl_loss: 56.87121224940098\n",
      "Epoch: 181, train_loss: 41.7, vsl_loss: 51.1247779793256\n",
      "Epoch: 182, train_loss: 49.4, vsl_loss: 60.673211607868375\n",
      "Epoch: 183, train_loss: 45.7, vsl_loss: 51.74876263531181\n",
      "Epoch: 184, train_loss: 32.6, vsl_loss: 53.034781246741524\n",
      "Epoch: 185, train_loss: 26.3, vsl_loss: 55.616886292041464\n",
      "Epoch: 186, train_loss: 42.9, vsl_loss: 50.70209877080663\n",
      "Epoch: 187, train_loss: 48.0, vsl_loss: 52.227478692505414\n",
      "Epoch: 188, train_loss: 37.0, vsl_loss: 51.9333072451827\n",
      "Epoch: 189, train_loss: 38.7, vsl_loss: 52.138186561021485\n",
      "Epoch: 190, train_loss: 61.7, vsl_loss: 55.439179503974984\n",
      "Epoch: 191, train_loss: 53.2, vsl_loss: 55.19289337951467\n",
      "Epoch: 192, train_loss: 40.2, vsl_loss: 55.096059151615286\n",
      "Epoch: 193, train_loss: 33.8, vsl_loss: 51.74218572005222\n",
      "Epoch: 194, train_loss: 51.9, vsl_loss: 56.208013699244056\n",
      "Epoch: 195, train_loss: 35.9, vsl_loss: 51.67980465712423\n",
      "Epoch: 196, train_loss: 42.3, vsl_loss: 49.92499524022757\n",
      "Epoch: 197, train_loss: 43.8, vsl_loss: 52.88957860233926\n",
      "Epoch: 198, train_loss: 26.1, vsl_loss: 50.274822800136825\n",
      "Epoch: 199, train_loss: 30.5, vsl_loss: 50.43309163313644\n",
      "Epoch: 200, train_loss: 40.8, vsl_loss: 50.642200056655476\n",
      "Epoch: 201, train_loss: 38.1, vsl_loss: 50.09271919179241\n",
      "Epoch: 202, train_loss: 25.6, vsl_loss: 58.44478919317398\n",
      "Epoch: 203, train_loss: 40.9, vsl_loss: 49.71939647112292\n",
      "Epoch: 204, train_loss: 34.2, vsl_loss: 49.79731261785177\n",
      "Epoch: 205, train_loss: 42.7, vsl_loss: 54.57927721418923\n",
      "Epoch: 206, train_loss: 32.8, vsl_loss: 62.102331134810996\n",
      "Epoch: 207, train_loss: 40.0, vsl_loss: 56.06047456219645\n",
      "Epoch: 208, train_loss: 37.0, vsl_loss: 50.08411765456168\n",
      "Epoch: 209, train_loss: 37.1, vsl_loss: 49.10457256757135\n",
      "Epoch: 210, train_loss: 46.9, vsl_loss: 50.786536937829865\n",
      "Epoch: 211, train_loss: 33.2, vsl_loss: 53.0152023521896\n",
      "Epoch: 212, train_loss: 36.7, vsl_loss: 53.764513847019494\n",
      "Epoch: 213, train_loss: 30.1, vsl_loss: 50.631748742063245\n",
      "Epoch: 214, train_loss: 30.4, vsl_loss: 50.23081482812634\n",
      "Epoch: 215, train_loss: 46.0, vsl_loss: 62.68885603177666\n",
      "Epoch: 216, train_loss: 40.8, vsl_loss: 50.760040860209514\n",
      "Epoch: 217, train_loss: 26.8, vsl_loss: 50.20559870399867\n",
      "Epoch: 218, train_loss: 52.2, vsl_loss: 51.516679559426244\n",
      "Epoch: 219, train_loss: 35.7, vsl_loss: 50.04035439705633\n",
      "Epoch: 220, train_loss: 41.0, vsl_loss: 49.09398935659471\n",
      "Epoch: 221, train_loss: 31.5, vsl_loss: 49.34425371828375\n",
      "Epoch: 222, train_loss: 39.3, vsl_loss: 48.90811493700175\n",
      "Epoch: 223, train_loss: 53.2, vsl_loss: 53.44407209587406\n",
      "Epoch: 224, train_loss: 22.5, vsl_loss: 52.08511600534182\n",
      "Epoch: 225, train_loss: 45.0, vsl_loss: 48.888514773957716\n",
      "Epoch: 226, train_loss: 28.2, vsl_loss: 52.85552872688305\n",
      "Epoch: 227, train_loss: 33.2, vsl_loss: 51.71564050220199\n",
      "Epoch: 228, train_loss: 41.6, vsl_loss: 50.56086520544789\n",
      "Epoch: 229, train_loss: 34.9, vsl_loss: 50.75487195836613\n",
      "Epoch: 230, train_loss: 56.6, vsl_loss: 47.54743369484935\n",
      "Epoch: 231, train_loss: 33.8, vsl_loss: 48.442322213523134\n",
      "Epoch: 232, train_loss: 34.4, vsl_loss: 48.06269426144809\n",
      "Epoch: 233, train_loss: 34.1, vsl_loss: 49.88727498258363\n",
      "Epoch: 234, train_loss: 38.1, vsl_loss: 49.035751137248035\n",
      "Epoch: 235, train_loss: 59.5, vsl_loss: 51.88748356048179\n",
      "Epoch: 236, train_loss: 28.3, vsl_loss: 49.31066540026725\n",
      "Epoch: 237, train_loss: 28.8, vsl_loss: 50.515677039198565\n",
      "Epoch: 238, train_loss: 35.8, vsl_loss: 47.26987141692903\n",
      "Epoch: 239, train_loss: 29.9, vsl_loss: 47.91538339392032\n",
      "Epoch: 240, train_loss: 45.2, vsl_loss: 54.4192482564727\n",
      "Epoch: 241, train_loss: 26.3, vsl_loss: 47.68397897238752\n",
      "Epoch: 242, train_loss: 25.4, vsl_loss: 53.11275858699652\n",
      "Epoch: 243, train_loss: 32.8, vsl_loss: 53.334165319507896\n",
      "Epoch: 244, train_loss: 20.3, vsl_loss: 48.0383604714928\n",
      "Epoch: 245, train_loss: 27.3, vsl_loss: 50.59996604700232\n",
      "Epoch: 246, train_loss: 21.0, vsl_loss: 47.042865501634104\n",
      "Epoch: 247, train_loss: 27.3, vsl_loss: 48.18009950112343\n",
      "Epoch: 248, train_loss: 33.2, vsl_loss: 49.64738273472815\n",
      "Epoch: 249, train_loss: 34.3, vsl_loss: 50.64113954433052\n",
      "Epoch: 250, train_loss: 34.5, vsl_loss: 48.628711691667085\n",
      "Epoch: 251, train_loss: 42.5, vsl_loss: 47.64396142488853\n",
      "Epoch: 252, train_loss: 35.9, vsl_loss: 48.31896116987718\n",
      "Epoch: 253, train_loss: 29.7, vsl_loss: 57.793959981118356\n",
      "Epoch: 254, train_loss: 34.0, vsl_loss: 47.73053635363196\n",
      "Epoch: 255, train_loss: 41.1, vsl_loss: 52.38526345667525\n",
      "Epoch: 256, train_loss: 30.4, vsl_loss: 47.972358219212005\n",
      "Epoch: 257, train_loss: 20.7, vsl_loss: 49.09396107162777\n",
      "Epoch: 258, train_loss: 32.4, vsl_loss: 46.51451065304815\n",
      "Epoch: 259, train_loss: 29.5, vsl_loss: 46.23324402748228\n",
      "Epoch: 260, train_loss: 29.4, vsl_loss: 52.75511846272216\n",
      "Epoch: 261, train_loss: 56.2, vsl_loss: 51.08518973527551\n",
      "Epoch: 262, train_loss: 24.3, vsl_loss: 47.94224021723547\n",
      "Epoch: 263, train_loss: 41.8, vsl_loss: 47.17672675750838\n",
      "Epoch: 264, train_loss: 39.0, vsl_loss: 48.435791485302104\n",
      "Epoch: 265, train_loss: 43.1, vsl_loss: 47.91590336909946\n",
      "Epoch: 266, train_loss: 30.9, vsl_loss: 47.2509135262254\n",
      "Epoch: 267, train_loss: 35.9, vsl_loss: 46.08398575754237\n",
      "Epoch: 268, train_loss: 40.8, vsl_loss: 56.11048850241776\n",
      "Epoch: 269, train_loss: 37.0, vsl_loss: 48.00611685579925\n",
      "Epoch: 270, train_loss: 21.2, vsl_loss: 46.2805178793736\n",
      "Epoch: 271, train_loss: 29.5, vsl_loss: 46.61404218084912\n",
      "Epoch: 272, train_loss: 29.7, vsl_loss: 49.288700952757395\n",
      "Epoch: 273, train_loss: 36.9, vsl_loss: 50.54548692359828\n",
      "Epoch: 274, train_loss: 37.2, vsl_loss: 46.85287306381248\n",
      "Epoch: 275, train_loss: 29.8, vsl_loss: 47.359824881195976\n",
      "Epoch: 276, train_loss: 29.4, vsl_loss: 45.82318584405612\n",
      "Epoch: 277, train_loss: 35.9, vsl_loss: 49.28273408647958\n",
      "Epoch: 278, train_loss: 38.6, vsl_loss: 48.93377844413719\n",
      "Epoch: 279, train_loss: 42.2, vsl_loss: 47.5520811478348\n",
      "Epoch: 280, train_loss: 26.6, vsl_loss: 47.60428183933093\n",
      "Epoch: 281, train_loss: 22.5, vsl_loss: 45.29284404072998\n",
      "Epoch: 282, train_loss: 30.2, vsl_loss: 49.63610652301089\n",
      "Epoch: 283, train_loss: 30.3, vsl_loss: 45.95673974591935\n",
      "Epoch: 284, train_loss: 29.9, vsl_loss: 48.04549779884506\n",
      "Epoch: 285, train_loss: 33.9, vsl_loss: 47.91332816515104\n",
      "Epoch: 286, train_loss: 25.2, vsl_loss: 45.297914331948185\n",
      "Epoch: 287, train_loss: 38.9, vsl_loss: 51.11078286900449\n",
      "Epoch: 288, train_loss: 34.3, vsl_loss: 47.519753635706515\n",
      "Epoch: 289, train_loss: 37.3, vsl_loss: 52.125841403305934\n",
      "Epoch: 290, train_loss: 34.0, vsl_loss: 46.575195873598545\n",
      "Epoch: 291, train_loss: 23.5, vsl_loss: 46.11407703063511\n",
      "Epoch: 292, train_loss: 25.5, vsl_loss: 48.69823639270456\n",
      "Epoch: 293, train_loss: 23.5, vsl_loss: 50.186107883154605\n",
      "Epoch: 294, train_loss: 35.0, vsl_loss: 45.87218019763187\n",
      "Epoch: 295, train_loss: 21.3, vsl_loss: 47.20029870349135\n",
      "Epoch: 296, train_loss: 34.8, vsl_loss: 46.52435899059056\n",
      "Epoch: 297, train_loss: 35.2, vsl_loss: 45.52494258915897\n",
      "Epoch: 298, train_loss: 38.2, vsl_loss: 46.26741811793548\n",
      "Epoch: 299, train_loss: 27.4, vsl_loss: 48.77445138047043\n",
      "Epoch: 300, train_loss: 27.8, vsl_loss: 46.59035915662796\n",
      "Epoch: 301, train_loss: 40.1, vsl_loss: 52.30885203366877\n",
      "Epoch: 302, train_loss: 37.0, vsl_loss: 45.4440055442627\n",
      "Epoch: 303, train_loss: 33.2, vsl_loss: 52.454662181473715\n",
      "Epoch: 304, train_loss: 38.0, vsl_loss: 52.502834957276754\n",
      "Epoch: 305, train_loss: 27.3, vsl_loss: 46.61414470176062\n",
      "Epoch: 306, train_loss: 52.1, vsl_loss: 44.735552143706165\n",
      "Epoch: 307, train_loss: 34.4, vsl_loss: 45.7047783127503\n",
      "Epoch: 308, train_loss: 24.2, vsl_loss: 46.82901295981399\n",
      "Epoch: 309, train_loss: 40.4, vsl_loss: 48.64097185203221\n",
      "Epoch: 310, train_loss: 26.0, vsl_loss: 45.54992446904302\n",
      "Epoch: 311, train_loss: 31.1, vsl_loss: 45.95008923816556\n",
      "Epoch: 312, train_loss: 42.0, vsl_loss: 45.275905503820425\n",
      "Epoch: 313, train_loss: 36.3, vsl_loss: 49.01377206114952\n",
      "Epoch: 314, train_loss: 38.6, vsl_loss: 45.556359950986305\n",
      "Epoch: 315, train_loss: 32.4, vsl_loss: 47.52931486879932\n",
      "Epoch: 316, train_loss: 26.7, vsl_loss: 45.431918234293356\n",
      "Epoch: 317, train_loss: 22.3, vsl_loss: 46.64317812110141\n",
      "Epoch: 318, train_loss: 21.3, vsl_loss: 44.7004106483412\n",
      "Epoch: 319, train_loss: 29.5, vsl_loss: 44.99245536804342\n",
      "Epoch: 320, train_loss: 30.9, vsl_loss: 48.74431851414912\n",
      "Epoch: 321, train_loss: 43.0, vsl_loss: 51.72004183207361\n",
      "Epoch: 322, train_loss: 31.2, vsl_loss: 46.43913355922857\n",
      "Epoch: 323, train_loss: 40.7, vsl_loss: 48.800236432268186\n",
      "Epoch: 324, train_loss: 27.0, vsl_loss: 46.17461043394674\n",
      "Epoch: 325, train_loss: 30.7, vsl_loss: 46.28022463458186\n",
      "Epoch: 326, train_loss: 15.9, vsl_loss: 47.01960515095369\n",
      "Epoch: 327, train_loss: 26.2, vsl_loss: 48.163347062969365\n",
      "Epoch: 328, train_loss: 23.2, vsl_loss: 44.44580448958994\n",
      "Epoch: 329, train_loss: 37.4, vsl_loss: 47.09251923678237\n",
      "Epoch: 330, train_loss: 28.6, vsl_loss: 51.49852709097594\n",
      "Epoch: 331, train_loss: 27.3, vsl_loss: 44.952621950650716\n",
      "Epoch: 332, train_loss: 17.8, vsl_loss: 44.289936417024045\n",
      "Epoch: 333, train_loss: 24.1, vsl_loss: 44.87117697879419\n",
      "Epoch: 334, train_loss: 28.0, vsl_loss: 45.78951046056953\n",
      "Epoch: 335, train_loss: 23.9, vsl_loss: 43.71671144310985\n",
      "Epoch: 336, train_loss: 32.1, vsl_loss: 45.5050350770818\n",
      "Epoch: 337, train_loss: 28.5, vsl_loss: 47.04824815904345\n",
      "Epoch: 338, train_loss: 19.5, vsl_loss: 46.40185920828791\n",
      "Epoch: 339, train_loss: 35.5, vsl_loss: 47.05971892512309\n",
      "Epoch: 340, train_loss: 25.1, vsl_loss: 46.16527578177312\n",
      "Epoch: 341, train_loss: 33.7, vsl_loss: 45.3513978363358\n",
      "Epoch: 342, train_loss: 30.2, vsl_loss: 48.690873156203274\n",
      "Epoch: 343, train_loss: 29.9, vsl_loss: 45.497927626974565\n",
      "Epoch: 344, train_loss: 28.5, vsl_loss: 46.317982259458866\n",
      "Epoch: 345, train_loss: 21.5, vsl_loss: 46.436093256460154\n",
      "Epoch: 346, train_loss: 26.7, vsl_loss: 54.865098750955084\n",
      "Epoch: 347, train_loss: 34.7, vsl_loss: 44.75541635933878\n",
      "Epoch: 348, train_loss: 28.0, vsl_loss: 46.86428650931425\n",
      "Epoch: 349, train_loss: 23.5, vsl_loss: 44.590333561643725\n",
      "Epoch: 350, train_loss: 24.6, vsl_loss: 46.8577463040915\n",
      "Epoch: 351, train_loss: 28.5, vsl_loss: 45.454117723613024\n",
      "Epoch: 352, train_loss: 31.9, vsl_loss: 43.9810588248808\n",
      "Epoch: 353, train_loss: 23.9, vsl_loss: 45.00602635944527\n",
      "Epoch: 354, train_loss: 28.0, vsl_loss: 44.0111666279851\n",
      "Epoch: 355, train_loss: 24.5, vsl_loss: 44.50794067094522\n",
      "Epoch: 356, train_loss: 26.9, vsl_loss: 44.92070029416073\n",
      "Epoch: 357, train_loss: 22.8, vsl_loss: 46.47873053747919\n",
      "Epoch: 358, train_loss: 32.2, vsl_loss: 48.789051399259485\n",
      "Epoch: 359, train_loss: 43.0, vsl_loss: 46.44591799856565\n",
      "Epoch: 360, train_loss: 32.6, vsl_loss: 49.503012782840706\n",
      "Epoch: 361, train_loss: 23.9, vsl_loss: 44.8543398676515\n",
      "Epoch: 362, train_loss: 29.8, vsl_loss: 46.031688720458334\n",
      "Epoch: 363, train_loss: 41.3, vsl_loss: 48.14327578800604\n",
      "Epoch: 364, train_loss: 24.0, vsl_loss: 51.59462143129678\n",
      "Epoch: 365, train_loss: 24.9, vsl_loss: 43.20374797915362\n",
      "Epoch: 366, train_loss: 19.6, vsl_loss: 43.75803974241812\n",
      "Epoch: 367, train_loss: 29.5, vsl_loss: 48.044334030996794\n",
      "Epoch: 368, train_loss: 26.5, vsl_loss: 51.63113776352227\n",
      "Epoch: 369, train_loss: 29.2, vsl_loss: 45.95010459976013\n",
      "Epoch: 370, train_loss: 28.0, vsl_loss: 44.81969687457113\n",
      "Epoch: 371, train_loss: 21.2, vsl_loss: 46.581661534601\n",
      "Epoch: 372, train_loss: 24.3, vsl_loss: 44.85032010268575\n",
      "Epoch: 373, train_loss: 30.7, vsl_loss: 44.114549178804594\n",
      "Epoch: 374, train_loss: 30.3, vsl_loss: 44.32513988414548\n",
      "Epoch: 375, train_loss: 18.8, vsl_loss: 45.27058028811574\n",
      "Epoch: 376, train_loss: 27.5, vsl_loss: 44.450309259426795\n",
      "Epoch: 377, train_loss: 29.2, vsl_loss: 44.05534750347938\n",
      "Epoch: 378, train_loss: 20.1, vsl_loss: 44.65825907324044\n",
      "Epoch: 379, train_loss: 27.6, vsl_loss: 44.0447295400851\n",
      "Epoch: 380, train_loss: 29.2, vsl_loss: 43.73676669927239\n",
      "Epoch: 381, train_loss: 25.4, vsl_loss: 44.04909843589595\n",
      "Epoch: 382, train_loss: 22.5, vsl_loss: 45.25764541701522\n",
      "Epoch: 383, train_loss: 24.5, vsl_loss: 44.61870729560141\n",
      "Epoch: 384, train_loss: 25.1, vsl_loss: 43.98070653366923\n",
      "Epoch: 385, train_loss: 24.2, vsl_loss: 46.93010454668764\n",
      "Epoch: 386, train_loss: 26.1, vsl_loss: 43.85802704651071\n",
      "Epoch: 387, train_loss: 27.3, vsl_loss: 45.71989567456228\n",
      "Epoch: 388, train_loss: 23.0, vsl_loss: 43.60761111715253\n",
      "Epoch: 389, train_loss: 27.3, vsl_loss: 43.477172732523606\n",
      "Epoch: 390, train_loss: 26.0, vsl_loss: 44.550080567529044\n",
      "Epoch: 391, train_loss: 32.7, vsl_loss: 44.123739449251744\n",
      "Epoch: 392, train_loss: 24.4, vsl_loss: 45.65527714597022\n",
      "Epoch: 393, train_loss: 25.4, vsl_loss: 45.212174958361345\n",
      "Epoch: 394, train_loss: 21.9, vsl_loss: 44.643413392677495\n",
      "Epoch: 395, train_loss: 21.1, vsl_loss: 45.63437802782167\n",
      "Epoch: 396, train_loss: 17.9, vsl_loss: 45.0507005975808\n",
      "Epoch: 397, train_loss: 18.1, vsl_loss: 45.22956510613763\n",
      "Epoch: 398, train_loss: 20.7, vsl_loss: 45.37645806504564\n",
      "Epoch: 399, train_loss: 24.0, vsl_loss: 44.44474445407284\n",
      "Epoch: 400, train_loss: 30.5, vsl_loss: 43.816798128697066\n",
      "Epoch: 401, train_loss: 17.7, vsl_loss: 47.577813861030094\n",
      "Epoch: 402, train_loss: 17.0, vsl_loss: 49.056583137864074\n",
      "Epoch: 403, train_loss: 35.0, vsl_loss: 45.47431247307664\n",
      "Epoch: 404, train_loss: 27.3, vsl_loss: 45.01493216026307\n",
      "Epoch: 405, train_loss: 19.4, vsl_loss: 44.45707715454282\n",
      "Epoch: 406, train_loss: 23.5, vsl_loss: 43.655102010305505\n",
      "Epoch: 407, train_loss: 20.6, vsl_loss: 45.043797100415546\n",
      "Epoch: 408, train_loss: 23.4, vsl_loss: 44.284119091841845\n",
      "Epoch: 409, train_loss: 30.4, vsl_loss: 46.212419790989145\n",
      "Epoch: 410, train_loss: 18.0, vsl_loss: 43.470846069306894\n",
      "Epoch: 411, train_loss: 23.8, vsl_loss: 42.973204771332306\n",
      "Epoch: 412, train_loss: 23.1, vsl_loss: 46.085878567922904\n",
      "Epoch: 413, train_loss: 22.2, vsl_loss: 49.66390995161937\n",
      "Epoch: 414, train_loss: 23.0, vsl_loss: 44.34073953863542\n",
      "Epoch: 415, train_loss: 17.6, vsl_loss: 44.723231591976436\n",
      "Epoch: 416, train_loss: 22.5, vsl_loss: 44.81298814347565\n",
      "Epoch: 417, train_loss: 31.4, vsl_loss: 47.27435082476651\n",
      "Epoch: 418, train_loss: 21.2, vsl_loss: 45.38969266484407\n",
      "Epoch: 419, train_loss: 21.8, vsl_loss: 44.22701169223944\n",
      "Epoch: 420, train_loss: 21.3, vsl_loss: 48.524186301267214\n",
      "Epoch: 421, train_loss: 35.4, vsl_loss: 43.649368099410715\n",
      "Epoch: 422, train_loss: 22.4, vsl_loss: 44.44876192093397\n",
      "Epoch: 423, train_loss: 23.3, vsl_loss: 44.72042826964824\n",
      "Epoch: 424, train_loss: 15.9, vsl_loss: 44.57187256848442\n",
      "Epoch: 425, train_loss: 18.1, vsl_loss: 43.929829189530636\n",
      "Epoch: 426, train_loss: 25.1, vsl_loss: 44.59078273178318\n",
      "Epoch: 427, train_loss: 24.1, vsl_loss: 46.97190270452433\n",
      "Epoch: 428, train_loss: 24.6, vsl_loss: 45.09655371709916\n",
      "Epoch: 429, train_loss: 33.7, vsl_loss: 43.84535566493018\n",
      "Epoch: 430, train_loss: 35.7, vsl_loss: 44.809949050421395\n",
      "Epoch: 431, train_loss: 18.3, vsl_loss: 46.81237295852212\n",
      "Epoch: 432, train_loss: 18.4, vsl_loss: 44.59807280673852\n",
      "Epoch: 433, train_loss: 21.1, vsl_loss: 44.49412921036941\n",
      "Epoch: 434, train_loss: 24.1, vsl_loss: 43.583188953368335\n",
      "Epoch: 435, train_loss: 40.5, vsl_loss: 43.93381583919318\n",
      "Epoch: 436, train_loss: 24.0, vsl_loss: 44.40023380646662\n",
      "Epoch: 437, train_loss: 26.8, vsl_loss: 44.450825012043886\n",
      "Epoch: 438, train_loss: 32.5, vsl_loss: 43.96946243765289\n",
      "Epoch: 439, train_loss: 24.9, vsl_loss: 47.700337235819795\n",
      "Epoch: 440, train_loss: 20.4, vsl_loss: 45.481972911952475\n",
      "Epoch: 441, train_loss: 25.1, vsl_loss: 44.502871564859596\n",
      "Epoch: 442, train_loss: 22.8, vsl_loss: 43.98302952205901\n",
      "Epoch: 443, train_loss: 20.5, vsl_loss: 44.91889657908775\n",
      "Epoch: 444, train_loss: 31.9, vsl_loss: 43.01529019208167\n",
      "Epoch: 445, train_loss: 18.7, vsl_loss: 48.58082779205332\n",
      "Epoch: 446, train_loss: 20.9, vsl_loss: 45.576092130305504\n",
      "Epoch: 447, train_loss: 23.8, vsl_loss: 43.881180440181296\n",
      "Epoch: 448, train_loss: 21.2, vsl_loss: 47.393094562131644\n",
      "Epoch: 449, train_loss: 21.7, vsl_loss: 43.256973104941956\n",
      "Epoch: 450, train_loss: 26.1, vsl_loss: 43.844505750459085\n",
      "Epoch: 451, train_loss: 24.5, vsl_loss: 43.539514144170404\n",
      "Epoch: 452, train_loss: 15.0, vsl_loss: 44.91473001398439\n",
      "Epoch: 453, train_loss: 25.0, vsl_loss: 44.078156319066046\n",
      "Epoch: 454, train_loss: 16.2, vsl_loss: 44.06559475819585\n",
      "Epoch: 455, train_loss: 16.9, vsl_loss: 44.247074041240836\n",
      "Epoch: 456, train_loss: 22.9, vsl_loss: 44.66499252896218\n",
      "Epoch: 457, train_loss: 23.4, vsl_loss: 42.97705531818564\n",
      "Epoch: 458, train_loss: 19.6, vsl_loss: 43.68809041487492\n",
      "Epoch: 459, train_loss: 25.1, vsl_loss: 43.97398889844161\n",
      "Epoch: 460, train_loss: 17.7, vsl_loss: 44.51701871505516\n",
      "Epoch: 461, train_loss: 23.5, vsl_loss: 43.889178746935734\n",
      "Epoch: 462, train_loss: 23.3, vsl_loss: 42.71403141945934\n",
      "Epoch: 463, train_loss: 17.7, vsl_loss: 45.63239172269991\n",
      "Epoch: 464, train_loss: 21.5, vsl_loss: 45.2769240291092\n",
      "Epoch: 465, train_loss: 20.2, vsl_loss: 44.090351149469775\n",
      "Epoch: 466, train_loss: 25.1, vsl_loss: 43.74203436879547\n",
      "Epoch: 467, train_loss: 14.6, vsl_loss: 43.49604058811201\n",
      "Epoch: 468, train_loss: 18.2, vsl_loss: 43.690803548956474\n",
      "Epoch: 469, train_loss: 19.1, vsl_loss: 43.6111536832569\n",
      "Epoch: 470, train_loss: 27.2, vsl_loss: 45.5152064534622\n",
      "Epoch: 471, train_loss: 18.3, vsl_loss: 44.636810787329615\n",
      "Epoch: 472, train_loss: 17.9, vsl_loss: 48.97370741432896\n",
      "Epoch: 473, train_loss: 31.8, vsl_loss: 47.73313261971597\n",
      "Epoch: 474, train_loss: 24.7, vsl_loss: 44.7300726445144\n",
      "Epoch: 475, train_loss: 18.1, vsl_loss: 43.4945627307942\n",
      "Epoch: 476, train_loss: 25.7, vsl_loss: 43.44965108529003\n",
      "Epoch: 477, train_loss: 21.0, vsl_loss: 42.96469366893814\n",
      "Epoch: 478, train_loss: 17.4, vsl_loss: 44.34234674284531\n",
      "Epoch: 479, train_loss: 24.6, vsl_loss: 43.54510674500791\n",
      "Epoch: 480, train_loss: 30.0, vsl_loss: 44.462646094452005\n",
      "Epoch: 481, train_loss: 19.1, vsl_loss: 47.22208174261612\n",
      "Epoch: 482, train_loss: 26.4, vsl_loss: 42.99006734498572\n",
      "Epoch: 483, train_loss: 22.4, vsl_loss: 46.585542051832824\n",
      "Epoch: 484, train_loss: 21.9, vsl_loss: 44.39464019792659\n",
      "Epoch: 485, train_loss: 23.0, vsl_loss: 43.714439998732935\n",
      "Epoch: 486, train_loss: 21.0, vsl_loss: 44.26846849991922\n",
      "Epoch: 487, train_loss: 22.5, vsl_loss: 44.05957790497675\n",
      "Epoch: 488, train_loss: 20.4, vsl_loss: 45.389785552098786\n",
      "Epoch: 489, train_loss: 23.8, vsl_loss: 47.72272081961855\n",
      "Epoch: 490, train_loss: 40.2, vsl_loss: 44.94854339857174\n",
      "Epoch: 491, train_loss: 19.4, vsl_loss: 44.4725529400514\n",
      "Epoch: 492, train_loss: 18.9, vsl_loss: 45.41405572811745\n",
      "Epoch: 493, train_loss: 19.1, vsl_loss: 45.54803758756724\n",
      "Epoch: 494, train_loss: 19.9, vsl_loss: 50.084702800328934\n",
      "Epoch: 495, train_loss: 20.6, vsl_loss: 42.977295084001994\n",
      "Epoch: 496, train_loss: 20.2, vsl_loss: 43.61085994727169\n",
      "Epoch: 497, train_loss: 23.1, vsl_loss: 42.81224202192138\n",
      "Epoch: 498, train_loss: 21.0, vsl_loss: 43.39749664747524\n",
      "Epoch: 499, train_loss: 16.9, vsl_loss: 43.701458171447534\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation data",
   "id": "2ab3395b19f93714"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:28:41.662257Z",
     "start_time": "2025-03-12T16:28:41.641360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluation_df_raw = pd.read_csv('evaluation_data.csv')\n",
    "evaluation_df_raw.head()"
   ],
   "id": "cf8d201bcccd6a16",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       dteday  season  yr  mnth  hr  holiday  weekday  workingday  weathersit  \\\n",
       "0  2011-01-20       1   0     1   0        0        4           1           1   \n",
       "1  2011-01-20       1   0     1   1        0        4           1           1   \n",
       "2  2011-01-20       1   0     1   2        0        4           1           1   \n",
       "3  2011-01-20       1   0     1   3        0        4           1           1   \n",
       "4  2011-01-20       1   0     1   4        0        4           1           1   \n",
       "\n",
       "   temp   atemp   hum  windspeed  \n",
       "0  0.26  0.2273  0.56     0.3881  \n",
       "1  0.26  0.2727  0.56     0.0000  \n",
       "2  0.26  0.2727  0.56     0.0000  \n",
       "3  0.26  0.2576  0.56     0.1642  \n",
       "4  0.26  0.2576  0.56     0.1642  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.3881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.1642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.1642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:31:12.961094Z",
     "start_time": "2025-03-12T16:31:12.943485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluation_df = evaluation_df_raw.copy()\n",
    "evaluation_df.drop(['dteday'], axis=1, inplace=True)\n",
    "evaluation_df.info()"
   ],
   "id": "e3d4b6255799455f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6493 entries, 0 to 6492\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   season      6493 non-null   int64  \n",
      " 1   yr          6493 non-null   int64  \n",
      " 2   mnth        6493 non-null   int64  \n",
      " 3   hr          6493 non-null   int64  \n",
      " 4   holiday     6493 non-null   int64  \n",
      " 5   weekday     6493 non-null   int64  \n",
      " 6   workingday  6493 non-null   int64  \n",
      " 7   weathersit  6493 non-null   int64  \n",
      " 8   temp        6493 non-null   float64\n",
      " 9   atemp       6493 non-null   float64\n",
      " 10  hum         6493 non-null   float64\n",
      " 11  windspeed   6493 non-null   float64\n",
      "dtypes: float64(4), int64(8)\n",
      "memory usage: 608.8 KB\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:34:29.762782Z",
     "start_time": "2025-03-12T16:34:29.755076Z"
    }
   },
   "cell_type": "code",
   "source": "X = torch.from_numpy(evaluation_df.values).cuda()",
   "id": "a0597358c884e129",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:37:13.360799Z",
     "start_time": "2025-03-12T16:37:13.345679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    preds = model(X)\n",
    "    preds = preds.squeeze(dim=1)"
   ],
   "id": "4b3b9224a8fff001",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:34:38.326693Z",
     "start_time": "2025-03-12T16:34:38.313101Z"
    }
   },
   "cell_type": "code",
   "source": "preds[:10]",
   "id": "3178fb32baf202a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  7.5814,   3.1401,   1.6336,   1.9837,   2.3719,   6.7590,  35.4545,\n",
       "         83.8046, 213.9973,  94.7091], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:37:19.389721Z",
     "start_time": "2025-03-12T16:37:19.378201Z"
    }
   },
   "cell_type": "code",
   "source": "np_preds = preds.cpu().numpy()",
   "id": "9865becbb153b38f",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:37:38.789656Z",
     "start_time": "2025-03-12T16:37:38.785091Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.DataFrame(np_preds)",
   "id": "d3ca3fd6a0d29add",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T16:39:23.544263Z",
     "start_time": "2025-03-12T16:39:23.498017Z"
    }
   },
   "cell_type": "code",
   "source": "df.to_csv('roda_Celiski_ukasiewicz_predykcje.csv', index=False)",
   "id": "86f335d4a7d95605",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cc15c5ea47f864f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
