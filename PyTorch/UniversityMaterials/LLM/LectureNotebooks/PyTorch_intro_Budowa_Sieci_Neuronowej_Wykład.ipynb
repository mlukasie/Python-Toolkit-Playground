{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY88NuVprDxy"
      },
      "source": [
        "#PyTorch Intro - Budowa Sieci Neuronowej - Wykład"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ6auP2WCnLW"
      },
      "source": [
        "##Przygotowanie środowiska\n",
        "Upewnij się, że notatnik jest uruchomiony na maszynie z GPU. Jeśli GPU nie jest dostępne zmień typ maszyny (Runtime | Change runtime type) i wybierz T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfwBYOmmnw0b"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1G8i4wTrfq6"
      },
      "source": [
        "Biblioteka PyTorch (`torch`) jest domyślnie zainstalowana w środowisku COLAB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDGaiuGLrapP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Wersja biblioteki PyTorch: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ6c3h4nryQJ"
      },
      "source": [
        "Sprawdzenie dostępnego urządzenia GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGyMG3_2rxIC"
      },
      "outputs": [],
      "source": [
        "print(f\"Dostępność GPU: {torch.cuda.is_available()}\")\n",
        "print(f\"Typ GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Budowa sieci neuronowych w PyTorch\n",
        "\n",
        "Sieci neuronowe składają się z warstw operujących na tensorach zawierająch przetwarzane dane.\n",
        "Warstwy są podstawowymi elementami składowymi sieci neuronowych.\n",
        "Przestrzeń nazw `torch.nn` zawiera implementację wielu warstw pozwalające budować sieci neuronowe o różnych architekturach (w pełni połączone, splotowe, rekurencyjne czy Transformer).\n",
        "Wszystkie warstwy z biblioteki PyTorch dziedziczą z klasy `nn.Module`.\n",
        "\n",
        "**UWAGA**: Samodzielnie zaimplementowane warstwy czy sieci złożone z wielu warstw muszą również dziedziczyć z klasy `nn.Module`.\n",
        "Pozwala to tworzyć sieci o złożonej architekturze, zawierające jako składowe prostsze sieci (moduły).\n"
      ],
      "metadata": {
        "id": "7IJMk8nlQT8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Warstwy\n",
        "\n",
        "Lista warstw zaimplementowana w bibliotece PyTorch: [link](https://pytorch.org/docs/stable/nn.html#module-torch.nn).\n",
        "\n",
        "\n",
        "Większość warstw wchodzących w skład sieci neuronowych jest parametryzowana - zawiera zestaw parametrów (wag). Parametry warstwy bądź modułu sieci neuronowej są dostępne przez metody `parameters()` lub `named_parameters()`.\n",
        "Parametry (wagi) modułów sieci są przechowywane jako tensory z domyślnie włączonym śledzeniem historii obliczeń (ustawiony atrybut `requires_grad`).\n",
        "Parametry modułów sieci są inicjalizowane losowo i optymalizowane w procesie uczenia sieci metodą spadku wzdłuż gradientu."
      ],
      "metadata": {
        "id": "UpMRHxismxYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "EMIIKc1qarjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Warstwa liniowa\n",
        "**Warstwa liniowa** `nn.Linear` stosuje przekształcenie liniowe (a właściwie afiniczne) na wejściowym tensorze $x$.\n",
        "$$\n",
        "y = x W^T + b\n",
        "$$\n",
        "gdzie $W$ jest macierzą wag a $b$ wektorem obiążenia (bias). Warsta liniowa jest elementem składowym perceptronu wielowarstwowego.\n",
        "Parametry (wagi) warstwy liniowej, i wszystkich innych parametryzowanych warstw, są inicjalizowane losowo i optymalizowane w procesie uczenia.\n",
        "\n",
        "Dokumentacja: [link](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n"
      ],
      "metadata": {
        "id": "5F-DQ2dQaxOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_layer_parameters(layer: nn.Module):\n",
        "    print(f\"Parametry warstwy/modułu:\")\n",
        "    for name, param in layer.named_parameters():\n",
        "        print(f\"{name}: {param.shape=}  {param.requires_grad=}\")"
      ],
      "metadata": {
        "id": "J2wCpRVb6P1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_layer = nn.Linear(in_features=10000, out_features=8)\n",
        "print(linear_layer)\n",
        "\n",
        "x = torch.rand((4, 10000))\n",
        "print(f\"\\nRozmiar wejściowy: {x.shape}\")\n",
        "y = linear_layer(x)\n",
        "print(f\"Rozmiar wyjściowy: {y.shape}\")\n",
        "\n",
        "print_layer_parameters(linear_layer)"
      ],
      "metadata": {
        "id": "HyAchJrLaxVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Warstwy nieliniowe\n",
        "\n",
        "Poniżej opisanych jest kilka wybranych funkcji nieliniowych zaimplementowanych w bibliotece PyTorch.\n",
        "Zauważmy, że przedstawione poniżej warstwy nieliniowe nie posiadają parametrów optymalizowanych w procesie uczenia.\n",
        "\n",
        "Lista wszystkich warstw nieliniowych w PyToch: [link](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)."
      ],
      "metadata": {
        "id": "tCythL35fk1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warstwa nieliniowości** `nn.ReLU` (*rectified linear unit*) stosuje do każdego elemetu tensora funkcję:\n",
        "$$\n",
        "\\mathrm{ReLU(x)} = \\max(0, x)\n",
        "$$\n",
        "\n",
        "Dokumentacja: [link](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)."
      ],
      "metadata": {
        "id": "hMMwZSXMdw2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "#Pomocnicza funkcja ilustrująca działanie warstwy nieliniowości\n",
        "def test_nonlinearity(nonlienar_layer: nn.Module, label:str):\n",
        "    x = torch.rand((10,))*10 - 5     # Losowy 10-elementowy tensor o wartościach z przedziału (-10, 10)\n",
        "    print(f\"{x=}\")\n",
        "    y = nonlienar_layer(x)\n",
        "    print(f\"{y=}\")\n",
        "\n",
        "    # Wizualizacja wyników\n",
        "    x = torch.linspace(-7, 7, 100)\n",
        "    y = nonlienar_layer(x)\n",
        "    figure(figsize=(3, 3))\n",
        "    plt.plot(x.detach().numpy(), y.detach().numpy())\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(label)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dGhxhZMbfs3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nonlinear_layer = nn.ReLU()\n",
        "\n",
        "test_nonlinearity(nonlinear_layer, \"ReLU\")\n",
        "print_layer_parameters(nonlinear_layer)"
      ],
      "metadata": {
        "id": "XzPAjxVydw8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warstwa nieliniowości** `nn.GELU` (*Gaussian Error Linear Unit*) stosuje do każdego elemetu tensora funkcję:\n",
        "$$\n",
        "\\mathrm{GELU(x)} = x \\cdot \\Phi \\left( x \\right) \\, ,\n",
        "$$\n",
        "gdzie $\\Phi$ jest dystrybuantą rozkładu normalnego.\n",
        "Dokumentacja: [link](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html#torch.nn.GELU)."
      ],
      "metadata": {
        "id": "tcScKKNPfo7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nonlinear_layer = nn.GELU()\n",
        "test_nonlinearity(nonlinear_layer, \"GELU\")\n",
        "print_layer_parameters(nonlinear_layer)"
      ],
      "metadata": {
        "id": "4VFzfxjXfpVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warstwa nieliniowości** `nn.SiLU` (*Sigmoid Linear Unit*, zwana również *swish*) stosuje do każdego elemetu tensora funkcję:\n",
        "$$\n",
        "\\mathrm{SiLU(x)} = x \\cdot \\sigma \\left( x \\right) \\, ,\n",
        "$$\n",
        "gdzie $\\sigma$ jest funkcją logistyczną (sigmoidalną) zdefiniowaną:\n",
        "$$\n",
        "\\sigma(x) = \\frac{1}{1+e^{-x}}\n",
        "$$\n",
        "Dokumentacja: [link](https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html)."
      ],
      "metadata": {
        "id": "1VSiRtctkEF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nonlinearz_layer = nn.SiLU()\n",
        "test_nonlinearity(nonlinearz_layer, \"SiLU\")\n",
        "print_layer_parameters(nonlinear_layer)"
      ],
      "metadata": {
        "id": "3-zwedI7kEMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warstwa nieliniowości** `nn.Sigmoid` stosuje do każdego elemetu tensora funkcję logistyczną (sigmoidalną):\n",
        "$$\n",
        "\\mathrm{Sigmoid(x)} = \\sigma(x) =  \\frac{1}{1+e^{-x}}\n",
        "$$\n",
        "Dokumentacja: [link](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html#torch.nn.Sigmoid)."
      ],
      "metadata": {
        "id": "f4bf0UEbnfdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nonlinearz_layer = nn.Sigmoid()\n",
        "test_nonlinearity(nonlinearz_layer, \"Sigmoid\")\n",
        "print_layer_parameters(nonlinear_layer)"
      ],
      "metadata": {
        "id": "zJw80FNGjBKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warstwa nieliniowości** `nn.Tanh`  stosuje do każdego elemetu tensora funkcję tangensa hiperbolicznego:\n",
        "$$\n",
        "\\mathrm{tanh(x)} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "$$\n",
        "Dokumentacja: [link](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh)."
      ],
      "metadata": {
        "id": "EfxRzv3WngDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nonlinearz_layer = nn.Tanh()\n",
        "test_nonlinearity(nonlinearz_layer, \"Tanh\")\n",
        "print_layer_parameters(nonlinear_layer)"
      ],
      "metadata": {
        "id": "9rxxtkZ4ngJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Warstwy normalizacji"
      ],
      "metadata": {
        "id": "yA5hFTGtHyND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizacja wsadu** stosowana jest do normalizacji wartości przetwarzanych wektorów cech poprzez odjęcie średniej i podzielenie przez odchylenie standardowe. Pozwala to ustabilizować i przyśpieszyć proces uczenia i może poprawić generalizację modelu. Często stosowana jest w sieciach splotowych. Do dobrego działania wymaga wykorzystania dużych wsadów podczas trenowania modelu.\n",
        "\n",
        "Warstwa normalizacji wsadu zdefiniowana jest wzorem:\n",
        "\n",
        "$$\n",
        "y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} \\cdot \\gamma + \\beta\n",
        "$$\n",
        "$\\gamma$ i $\\beta$ są wyuczonymi parametrami rozmiaru $C$, gdzie $C$ jest liczbą cech albo kanałów wejścia.\n",
        "* **W fazie treningu** - wartość oczekiwana i wariancja obliczane są po elementach wsadu. Dodatkowo obliczana i zapamiętywana jest średnia krocząca wartości oczekiwanej i wariancji dla wszystkich przetworzonych wsadów.\n",
        "* **W fazie inferencji** - wykorzystywana jest wyznaczona w fazie treningu średnia krocząca wartości oczekiwanej i wariancji.\n",
        "\n",
        "Warstwa `nn.BatchNorm1d` akceptuje tensor rozmiaru $(N, C)$ lub $(N, C, L)$ i zwraca tensor tego samego rozmiaru. Dokumentacja: [link](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d).\n",
        "\n",
        "Do normalizacji danych obrazowych stosowana jest warstwa `nn.BatchNorm2d` przetwarzająca mapy cech wizyjnych rozmiaru $(N,C,H,W)$."
      ],
      "metadata": {
        "id": "dADL3qXmLxtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_batchnorm_params(layer: nn.Module):\n",
        "    print(f\"Parametry warstwy:\")\n",
        "    print(f\"Średnia krocząca wartości oczekiwanej: {layer.running_mean}\")\n",
        "    print(f\"Średnia krocząca wariancji: {layer.running_var}\")"
      ],
      "metadata": {
        "id": "lH6rJMdGZcvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defnicja warstwy BatchNorm1d\n",
        "num_features = 5                # Liczba cech (kanałów)\n",
        "batch_norm = nn.BatchNorm1d(num_features)\n",
        "# Domyślnie warstwa tworzona jest w trybie treningowym (atrybut training ustawiony na True)\n",
        "print(f\"Tryb treningowy: {batch_norm.training=}\")\n",
        "print_batchnorm_params(batch_norm)\n",
        "\n",
        "# Utwórz losowy tensor\n",
        "batch_size = 3\n",
        "input_tensor = torch.randn(batch_size, num_features)\n",
        "\n",
        "print(\"\\nWejściowy tensor:\")\n",
        "print(input_tensor)\n",
        "\n",
        "# Zastosuj BatchNorm1d\n",
        "output_tensor = batch_norm(input_tensor)\n",
        "\n",
        "print(\"\\nPo zastosowaniu normalizacji wsadu:\")\n",
        "print(output_tensor)\n",
        "\n",
        "print(\"\\nŚrednie po wymiarze 0 (rozmiar wsadu):\")\n",
        "print(output_tensor.mean(dim=0))\n",
        "print(f\"Odchylenie standardowe po wymiarze 0 (rozmiar wsadu): {output_tensor.std(dim=0)}\")\n",
        "print()\n",
        "\n",
        "print(\"\\nŚrednie po wymiarze 1 (elementy wektorów cech/kanały):\")\n",
        "print(output_tensor.mean(dim=1))\n",
        "print(f\"Odchylenie standardowe po wymiarze 1 (elementy wektorów cech/kanały): {output_tensor.std(dim=1)}\")\n",
        "print()\n",
        "\n",
        "\n",
        "print_batchnorm_params(batch_norm)"
      ],
      "metadata": {
        "id": "KPScLBa4Lx0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizacja warstwy** `nn.LayerNorm` podobnie jak `nn.BatchNorm1d` stosowana jest do normalizacji wartości przetwarzanych wektorów cech poprzez odjęcie średniej i podzielenie przez odchylenie standardowe.\n",
        "W odróżnieniu od warstwy `nn.BatchNorm1d` normalizacja przebiega po wymiarze cech (kanałów). Wartość oczekiwana i średnia wyznaczana jest dla każdego elementu wsadu z osobna.\n",
        "Normalizacja warstwy często stosowana jest w sieciach rekurencyjnych lub opartych o architekturę Transformer.\n",
        "\n",
        "Zdefiniowana jest wzorem:\n",
        "\n",
        "$$\n",
        "y = \\frac{x - \\mathbb{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} \\cdot \\gamma + \\beta \\, ,\n",
        "$$\n",
        "gdzie $\\gamma$ i $\\beta$ są wyuczonymi parametrami.\n",
        "\n",
        "Dokumentacja: [link](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html#torch.nn.LayerNorm)"
      ],
      "metadata": {
        "id": "96Zt44Y-LyBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defnicja warstwy LayerNorm\n",
        "\n",
        "num_features = 5                # Liczba cech (kanałów)\n",
        "layer_norm = nn.LayerNorm(num_features)\n",
        "\n",
        "# Utwórz losowy tensor\n",
        "batch_size = 3\n",
        "input_tensor = torch.randn(batch_size, num_features)\n",
        "\n",
        "print(\"\\nWejściowy tensor:\")\n",
        "print(input_tensor)\n",
        "\n",
        "# Zastosuj BatchNorm1d\n",
        "output_tensor = layer_norm(input_tensor)\n",
        "\n",
        "print(\"\\nPo zastosowaniu normalizacji warstwy:\")\n",
        "print(output_tensor)\n",
        "\n",
        "print(\"\\nŚrednie po wymiarze 0 (rozmiar wsadu):\")\n",
        "print(output_tensor.mean(dim=0))\n",
        "print(f\"Odchylenie standardowe po wymiarze 0 (rozmiar wsadu): {output_tensor.std(dim=0)}\")\n",
        "print()\n",
        "\n",
        "print(\"\\nŚrednie po wymiarze 1 (elementy wektorów cech/kanały):\")\n",
        "print(output_tensor.mean(dim=1))\n",
        "print(f\"Odchylenie standardowe po wymiarze 1 (elementy wektorów cech/kanały): {output_tensor.std(dim=1)}\")\n",
        "print()\n"
      ],
      "metadata": {
        "id": "fhk5uVr2LyIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warstwa odrzutu** `nn.Dropout` jest metodą regularyzacji wykorzystywaną w celu ograniczenia przeuczenia i polepszenia generalizacji sieci neuronowej.\n",
        "\n",
        "*   **W fazie treningu** zerowane są losowo wybrane elementy przetwarzanego tensora. Elementy przetwarzanego tensora wygaszane są niezależnie, każdy z niewielkim prawdopodobieństwem $p$. Dodatkowo wyjście jest skalowane ze współczynnikiem $\\frac{1}{1-p}$.\n",
        "*   **W fazie inferencji** warstwa jest przekształceniem identycznościowym.\n",
        "\n",
        "Dokumentacja: [link](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)."
      ],
      "metadata": {
        "id": "gdOKSFNwH4ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inne warstwy\n",
        "\n",
        "Warstwa `nn.Flatten` pozwala zlinearyzować (spłaszczyć) wejściowy tensor do mniejszej liczby wymiarów.\n",
        "Na przykład może zostać wykorzystana aby przekształcić obraz danych jako tensor $(n=4, c=3, h=64, w=64)$, gdzie $n$ jest rozmiarem wsadu, $c$ liczbą kanałów a $h, w$ to rodzielczość do rozmiarów $(n=4, c \\cdot h \\cdot w = 12\\, 228)$.\n",
        "\n",
        "Dokumentacja: [link](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)."
      ],
      "metadata": {
        "id": "M_6HOSAUZDWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_height = 64\n",
        "image_width = 64\n",
        "image_tensor = torch.rand((4, 3, image_height, image_width))\n",
        "print(f\"Rozmiar wejściowy: {image_tensor.shape}\")\n",
        "\n",
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(image_tensor)\n",
        "print(f\"Rozmiar wejściowy: {flat_image.shape}\")\n"
      ],
      "metadata": {
        "id": "8S-5XoRRaSkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warstwa sekwencyjna** `nn.Sequential` pozwala zgrupować wiele połączonych sekwencyjnie warstw i traktować je jak jedną warstwę. Dane są przetwarzane przez poszczególne moduły w kolejności zdefiniowanej przy tworzeniu warstwy sekwencyjnej.\n",
        "\n",
        "Przykład - **perceptron wielowarstwowy** (MLP - *multi layer perceptron*) składa się z kilku warstw liniowych połączonych warstwami nieliniowymi (np. `nn.ReLU`). Perceptron wielowarstwowy możemy zaimplementować jako warstwę sekwencyjną złożoną z trzech warstw liniowych oddzielonych nieliniowością `nn.ReLU`.\n",
        "\n",
        "Zastosowanie warstw nieliniowych jest konieczne aby perceptron definiował nieliniową funkcję wejściowego tensora. Bezpośrednie połączenie kilku warstw liniowych, bez nieliniowości, jest przekształceniem linowym (może zostać zastąpione równoważną pojedynczą warstwą `nn.Linear`)."
      ],
      "metadata": {
        "id": "xtGxxu_-s4Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_mlp = nn.Sequential(\n",
        "    nn.Linear(10000, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 4)\n",
        "    )\n",
        "\n",
        "print(my_mlp)\n",
        "print()\n",
        "\n",
        "input_tensor = torch.rand(2, 10000)\n",
        "print(f\"{input_tensor.shape=}\")\n",
        "logits = my_mlp(input_tensor)\n",
        "print(f\"{logits.shape=}\")"
      ],
      "metadata": {
        "id": "9-nil1NKs4UZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Warstwa softmax** `nn.Softmax` stosuje funkcję Softmax wzdłuż podanego wymiaru wejściowego tensora. Stosowana do przekształcenia nieznormalizowanych wartości rzeczywistych zwracach przez sieć (zwanych logitami) w rozkład prawdopodobieństwa klas. Wynikowe wartości są z zakresu $[0, 1]$ i sumują się do $1$.\n",
        "\n",
        "$$\\mathrm{Softmax} \\left( x_i \\right)\n",
        "=\n",
        "\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n",
        " $$\n",
        "\n",
        "**UWAGA:** Warstwa Softmax nie powinna być używana wewnątrz modułu klasyfikatora do wyznaczenia rozkładu prawdopodobieństwa klas.\n",
        "Klasyfikator oparty o sieć neuronową powinien zwracać nieznormalizowane wartości rzeczywiste (logity). Aby zwiększyć stabilność numeryczną funkcja straty `nn.CrossEntropyLoss` wykorzystywana w treningu klasyfikatorów oczekuje na wejściu nieznormalizowanych wartości (logitów) a NIE rozkładu prawdopodobieństwa.\n",
        "\n",
        "\n",
        "Dokumentacja: [link](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html#softmax)."
      ],
      "metadata": {
        "id": "P0MBo46lvJnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{logits=}\")\n",
        "print(f\"Suma wartości logitów w wierszach: {logits.sum(dim=1)} \\n\")\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "probabilities = softmax(logits)\n",
        "print(f\"{probabilities}\")\n",
        "print(f\"Suma wartości w wierszach po zastosowaniu softmax: {probabilities.sum(dim=1)=}\\n\")\n",
        "\n",
        "print_layer_parameters(softmax)"
      ],
      "metadata": {
        "id": "USoFhauSvJt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Przykład: Perceptron wielowarstwowy\n",
        "\n",
        "W tym przykładzie zastosujemy **perceptron wielowarstwowy** do rozwiązania problemu klasyfikacji zbioru zawierającego syntetycznie wygenerowane punkty danych $x \\in \\mathbb{R}^{10}$ należące do dwóch klas.\n",
        "We wcześniejszym notatniku do klasyfikacje tych danych zastosowaliśmy metodę regresji logistycznej, zaimplementowaną z wykorzystaniem pojedycznej warstwy liniowej. Ponieważ punkty danych nie były liniowo separowalne skuteczność regresji logitycznej była ograniczona."
      ],
      "metadata": {
        "id": "Kzr0qb_Yi8uG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Urządzenie: {}\".format(device))"
      ],
      "metadata": {
        "id": "Wzdr-v9zZ-j8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generowanie syntetycznych danych\n",
        "Wygenerujmy zbiór zawierający syntetycznie wygeneorwane punkty danych $x \\in \\mathbb{R}^{10}$ należące do dwóch klas korzystając z funkcji `make_classification` z biblioteki scikit-learn ([link](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html))."
      ],
      "metadata": {
        "id": "hdhZTTm_o1z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Wygeneruj syntetyczny zbiór danych\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, n_informative=5, random_state=39)\n",
        "\n",
        "# Podziel na zbiór treningowy i testowy\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(f\"Liczba elementów w zbiorze danych: {X.shape[0]}\")\n",
        "print(f\"Liczba cech: {X.shape[1]}\")\n",
        "print(f\"Liczba klas: {len(np.unique(y))}\")"
      ],
      "metadata": {
        "id": "PBNGU9eARrWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[:2])\n",
        "print(y[:2])"
      ],
      "metadata": {
        "id": "wzozkuPd1M3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wizualizacja dwóch wybranych cech. Możemy zauważyć, że dla dwóch losowo wybranych cech elementy z różnych klas nie są liniowo separowalne."
      ],
      "metadata": {
        "id": "dYZ7hyTo-nD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Wybierz dwie losowe cechy\n",
        "f1 = random.randint(0, X.shape[1]-1)\n",
        "f2 = random.randint(0, X.shape[1]-1)\n",
        "\n",
        "plt.scatter(X[y==0, f1], X[y==0, f2], color='blue', label='Klasa 0', alpha=0.7)\n",
        "plt.scatter(X[y==1, f1], X[y==1, f2], color='red', label='Klasa 1', alpha=0.7)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.xlabel(f\"Wartość cechy {f1}\")\n",
        "plt.ylabel(f\"Wartość cechy {f2}\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i0eL9Jz7AEDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Implementacja sieci neuronowej\n",
        "\n",
        "W PyTorch wszystkie klasy implementujące moduły sieci neuronowej dziedziczą z klasy `nn.Module`. Jeśli piszemy własną klasę definiującą moduł sieci neuronowej **musi ona również dziedziczyć z klasy `nn.Module`**.\n",
        "Utworzymy klasę `MyMLP` implementujący wielowarwarstwowy perceptron złożony z dwóch warstw liniowych `nn.Linear` i nieliniowości `ReLU`.\n",
        "Aby zmniejszyć ryzyko przeuczenia i poprawić generalizację sieci przed ostatnią warstwą liniową zastosujemy warstwę odrzutu `nn.Dropout`.\n",
        "\n",
        "*   W metodzie `__init__` inicjalizujemy warstwy i moduły wchodzących w skład sieci. Nic nie stoi na przeszkodzie aby częścią składową sieci była inna, wcześniej utworzona sieć.\n",
        "*   W metodzie `forward` definiujemy logikę przetwarzania danych przez sieć. Jakie operacje i w jakiej kolejności są wykonywane na wejściowych danych. Najczęściej metoda `forward` otrzymuje jako argument pojedyczny tensor (ale teoretycznie może otrzymać zestaw kilku tensorów) i zwraca wynikowy tensor (lub tensory).\n",
        "\n",
        "**UWAGA:** Kolejność definicji warstw w metodzie `__init__` nie określa kolejności w jakiej dane będą przetwarzane przez kolejne warstwy. Logikę przetwarzania dediniujemy w metodzie `forward`."
      ],
      "metadata": {
        "id": "NUl4b_YFWJG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyMLP(nn.Module):\n",
        "    def __init__(self, n_features: int, n_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(n_features, 5)\n",
        "        self.fc2 = nn.Linear(5, n_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mxCWP37QdGZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utworzymy instancję klasy `MyMLP`. Jako argumenty podamy liczbę cech w zbiorze danych (`n_features`=10) oraz liczbę klas (`n_classes`=2)\n",
        "Dla porównaniu utworzymy również prostą sieć złożoną z jednej warstwy liniowej.\n",
        "Zauważmy, że zarówno dostępna w bibliotece PyTorch warstwa `nn.Linear` jak i zaimplementowana przez nas klasa `MyMLP` są pochodnymi klasy `nn.Module`."
      ],
      "metadata": {
        "id": "ODQLE0WbdmO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "n_features = X.shape[1]     # Liczna cech\n",
        "\n",
        "mlp_net = MyMLP(n_features, 2)\n",
        "linear_net = nn.Linear(n_features, 2)\n",
        "\n",
        "print(f\"{isinstance(linear_net, nn.Module)=}\")\n",
        "print(f\"{isinstance(mlp_net, nn.Module)=}\")"
      ],
      "metadata": {
        "id": "JdWkxGLoy3Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mlp_net)"
      ],
      "metadata": {
        "id": "82njTgC5llH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(linear_net)"
      ],
      "metadata": {
        "id": "MmhuJttJlsle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sprawdźmy działanie obu sieci. Każda z nich po przetworzeniu wsadu wektorów cech $X \\in \\mathbb{R}^{N \\times 10}$ zwraca tensor liczb rzeczywistych rozmiaru $(N, 2)$.\n",
        "Zauważmy, że tensor `z` zawierający wynikowe wartości ma włączone śledzenie obliczeń (`requires_grad=True`).\n",
        "\n",
        "**UWAGA:** Aby przetworzyć dane przez moduł sieci NIE należy wywoływać bezpośrednio metody `forward`. Podajemy dane bezpośrednio do modelu, na przykład `my_model(X)`. Pozwala to oprócz przejścia w przód (*forward*) wykonać dodatkowe operacje niezbędne do prawidłowego działania sieci."
      ],
      "metadata": {
        "id": "jpfz6yBUFFcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zamień dane (jako tablice ndarray) na tensory\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "z = linear_net(X_train_tensor)\n",
        "print(\"linear_net:\")\n",
        "print(f\"Rozmiar wejścia: {X_train_tensor.shape}\")\n",
        "print(f\"Rozmiar wyjścia: {z.shape}\")\n",
        "print(f\"Wyjście (pierwsze 5 elementów): {z[:5]}\")\n",
        "print(f\"{z.requires_grad=}\\n\")\n",
        "\n",
        "z = mlp_net(X_train_tensor)\n",
        "print(\"mlp_net:\")\n",
        "print(f\"Rozmiar wejścia: {X_train_tensor.shape}\")\n",
        "print(f\"Rozmiar wyjścia: {z.shape}\")\n",
        "print(f\"Wyjście (pierwsze 5 elementów): {z[:5]}\")\n",
        "print(f\"{z.requires_grad=}\")"
      ],
      "metadata": {
        "id": "vVwTzgt2Joqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "W naszym przypadku wyjścia z sieci możemy je traktować jako nieznormalizowane rozkłady prawdopodobieństwa każdej klasy.\n",
        "W literaturze poświęconej głębokiemu uczeniu nieznormalizowane wyjścia z sieci przyjmujące wartości rzeczywiste z zakresu $\\left( -\\infty, \\infty \\right)$ zwane są **logitami**.\n",
        "Aby zamienić wyjścia z sieci (logity) na rozkład prawdopodobieństwa klas należy zastosować funkcję softmax.\n",
        "\n",
        "**WAŻNE**: Nie należy normalizować wartości zwracanych przez sieć, na przykład stosując funkcję softmax lub sigmoid  wewnątrz modułu sieci. Ze względu na stabilność numeryczną funkcje straty zaimplementowane w PyTorch, takie jak entropia krzyżowa, domyślnie operują na nieznormalizowanych wartościach (logitach)."
      ],
      "metadata": {
        "id": "aLyJ1KOdiRIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Surowe wyjścia z sieci (logity)\")\n",
        "print(f\"{z=}\")\n",
        "print(f\"\\nWyjścia z sieci po normalizacji (softmax)\")\n",
        "print(f\"{torch.nn.functional.softmax(z)=}\")\n"
      ],
      "metadata": {
        "id": "Fd_Hjd4ZiRRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(logits: torch.Tensor, y_true: torch.Tensor) -> float:\n",
        "    # Oblicz dokładność klasyfikacji na podstawie zwróconych przez sieć nieznormalizowanych wartości (logitów)\n",
        "    y_pred_labels = torch.argmax(logits, dim=1).float()\n",
        "    accuracy = (y_pred_labels == y_true).sum().item() / y_true.size(0)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "xZmHNdlgZkuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funkcja `train_network` implementuje prostą pętlę treningową modelu sieci neuronowej.\n",
        "\n",
        "*   Trening modelu jest podzielony na **epoki**. W czasie jednej epoki następuje jednokrotne przejście przez zbiór danych.\n",
        "*   Zwykle w każdej epoce iteracyjnie przetwarzane są kolejne **wsady** ze zbioru danych. W naszym przypadku, ponieważ zbiór danych jest względnie mały, wsad zawiera wszystkie dane.\n",
        "*   Przetworzenie jednego wsadu (w naszym przypadku wszystkich danych) składa się z następujących kroków:\n",
        "    * Przetworzenie elementów wsadu przez sieć (przejście w przód): `logits = model(X_train_tensor)`\n",
        "    *   Wyznaczenie wartość funkcji straty `loss = criterion(logits, y_train_tensor)`\n",
        "    *   Wyznaczanie gradientu funkcji straty względem parametrów sieci (przejście w tył): `loss.backward()`\n",
        "    *   Krok optymalizacji parametrów sieci: `optimizer.step()`\n",
        "\n",
        "Jako funkcję straty wykorzystamy `CrossEntropyLoss`, łączącą w jednej klasie funkcję Softmax i funkcję straty binarnej entropii krzyżowej (patrz: [link](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.CrossEntropyLoss)).\n",
        "\n",
        "**WAŻNE - specyfika PyTorcha:**\n",
        "\n",
        "*   Domyślnie gradienty zapamiętywane w tensorach podlegających optymalizacji (wagach sieci) akumulują się po każdym wywołaniu przejścia w tył (metody `backward`). Dlatego konieczne jest wyzerowanie gradientów poleceniem `optimizer.zero_grad()`. Więcej informacji: [link](https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch).\n",
        "*   Moduł sieci neuronowej może być w trybie treningowym (`train`) lub ewaluacyjnym (`eval`). W fazie trenowania model musi zostać przełączony w tryb treningowy poleceniem `model.train()`. Przy wykorzystaniu modelu do wnioskowania (np. przy wyznaczaniu dokładności klasyfikacji na zbiorze testowym) należy przełączyć model w tryb ewaluacji poleceniem `model.eval()`.\n",
        "Różnice między trybem `train` i `eval`: [link](https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch).\n",
        "*   Podczas wyznaczania dokładności klasyfikacji na zbiorze testowym nie musimy budować grafu obliczeń - nie potrzebujemy wyznaczać gradientu funkcji straty. Wyłączymy budowania grafu obliczeń przy użyciu menadżera kontekstu\n",
        "`with torch.no_grad():` co zmniejszy wykorzystanie zasobów obliczeniowych.\n"
      ],
      "metadata": {
        "id": "HBMUSHaR5HAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_network(model: nn.Module, criterion: nn.Module, optimizer: torch.optim.Optimizer,\n",
        "                  eta: float = 0.2, n_epochs: int = 200):\n",
        "    model.train()\n",
        "    for k in range(n_epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(X_train_tensor)\n",
        "        # logits ma rozmiar (N, 1)\n",
        "        logits = logits.squeeze(1)\n",
        "        # logits ma rozmiar (N,)\n",
        "\n",
        "        # Wyznacz wartość funkcji straty\n",
        "        loss = criterion(logits, y_train_tensor)\n",
        "\n",
        "        # Przejście w tył- wyznaczenie gradientu funkcji straty względem parametrów (wag) modelu\n",
        "        loss.backward()\n",
        "\n",
        "        # Krok optymalizacji metodą spadku wzdłuż gradientu\n",
        "        optimizer.step()\n",
        "\n",
        "        train_accuracy = calc_accuracy(logits, y_train_tensor)\n",
        "\n",
        "        # Oblicz dokładność klasyfikacji na zbiorze testowym\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            logits = model(X_test_tensor)\n",
        "            model.train()\n",
        "            test_accuracy = calc_accuracy(logits.squeeze(1), y_test_tensor)\n",
        "\n",
        "        if k % 20 == 0:\n",
        "            print(f\"Epoch: {k}   Wartość funkcji straty: {loss.item():.5f}   Dokładność (train): {train_accuracy:.4f}   Dokładność (test): {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "U7FH5E1wYpg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trening i ewaluacja klasyfikatora `linear_net` opartego o pojedynczą warstwę liniową (regresja logistyczna)."
      ],
      "metadata": {
        "id": "O4u-EwK5Pi6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(linear_net.parameters(), lr=0.1)\n",
        "train_network(linear_net, criterion, optimizer)"
      ],
      "metadata": {
        "id": "6WDIz_TPAREJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trening i ewaluacja klasyfikatora opartego o perceptron wielowarstwowy `mlp_net`."
      ],
      "metadata": {
        "id": "WDgVW_zLPvDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(mlp_net.parameters(), lr=0.1)\n",
        "train_network(mlp_net, criterion, optimizer)"
      ],
      "metadata": {
        "id": "Wp4EOHHea5YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dla porównania sprawdzimy skuteczność klasyfikatora regresji logistycznej z biblioteki scikit-learn."
      ],
      "metadata": {
        "id": "w7KJ_ZKv4O97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_train_pred = model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "y_test_pred = model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Dokładność (train): {train_accuracy:.4f}   Dokładność (test): {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "9OJpZzEw3swE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}