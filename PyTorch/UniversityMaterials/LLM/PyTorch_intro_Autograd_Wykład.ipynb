{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY88NuVprDxy"
      },
      "source": [
        "#PyTorch Intro - Autoróżniczkowanie i Graf Obliczeń - Wykład\n",
        "\n",
        "Do optymalizacji parametrów (wag) sieci neuronowej podczas treningu modelu wykorzystuje się **metodę stochastycznego spadku wzdłuż gradientu**.\n",
        "Gradient funkcji straty względem parametrów sieci wyznaczany jest algorytmem **propagacji wstecznej** (*back propagation*).\n",
        "\n",
        "Sieć neuronową możemy potraktować jak złożoną funkcję mapującą wejściowe dane $x \\in \\mathcal{X}$ (np. obraz czy sekwencję audio) na wyjście $y \\in \\mathcal{Y}$ parametryzowaną zestawem parametrów (wag) $\\theta$.\n",
        "$$\n",
        "f_{\\theta}( x ) = y\n",
        "$$\n",
        "W przypadku $n$-klasowego klasyfikatora wyjściem z sieci jest wektor $y \\in \\mathbb{R}^n$ nieznormalizowanych wartości, zwanych logitami, z których możemy wyznaczyć rozkład prawdopodobieństwa klas korzystając z funkcji softmax.\n",
        "\n",
        "W jednym kroku treningu sieci neuronowych wykonujemy:\n",
        "1. **Przejście w przód** - przetworzenie zestawu wejściowych danych treningowych przez sieć i wyznaczenie wartości wynikowych $y = f_{\\theta}(x)$. Następnie wyznaczenie wartości funkcji straty\n",
        "$\\mathcal{L}$\n",
        "w oparciu o wynikową wartość z sieci i prawdziwą (docelową) wartość.\n",
        "2. **Przejście w tył** (propagacja wsteczna) - wyznaczenie **gradientu funkcji  straty** $\\mathcal{L}$ **względem parametrów sieci** $\\theta$.\n",
        "3. Krok optymalizacji parametrów sieci - zmiana w kierunku przeciwnym do gradientu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ6auP2WCnLW"
      },
      "source": [
        "##Przygotowanie środowiska\n",
        "Upewnij się, że notatnik jest uruchomiony na maszynie z GPU. Jeśli GPU nie jest dostępne zmień typ maszyny (Runtime | Change runtime type) i wybierz T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfwBYOmmnw0b"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1G8i4wTrfq6"
      },
      "source": [
        "Biblioteka PyTorch (`torch`) jest domyślnie zainstalowana w środowisku COLAB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDGaiuGLrapP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "print(f\"Wersja biblioteki PyTorch: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ6c3h4nryQJ"
      },
      "source": [
        "Sprawdzenie dostępnego urządzenia GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGyMG3_2rxIC"
      },
      "outputs": [],
      "source": [
        "print(f\"Dostępność GPU: {torch.cuda.is_available()}\")\n",
        "print(f\"Typ GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalacja pakietu torchviz do wizualizacji grafów obliczeń ([link](https://github.com/szagoruyko/pytorchviz))."
      ],
      "metadata": {
        "id": "JrekpMsKcafs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchviz"
      ],
      "metadata": {
        "id": "SzkIy9W8cZxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cazrrG01sbUH"
      },
      "source": [
        "#Automatyczne różniczkowanie (`torch.autograd`)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient** (lub gradientowe pole wektorowe) funkcji skalarnej wielu zmiennych $\n",
        "f: \\mathbb{R}^D → \\mathbb{R}\n",
        "$ oznaczamyy\n",
        "$\\nabla f$ (czytaj: nabla).\n",
        "W układzie współrzędnych kartezjańskich gradient jest wektorem, którego składowe są pochodnymi cząstkowymi funkcji $f$:\n",
        "$$\\nabla f=\\left[{\\frac {\\partial f}{\\partial x_{1}}},\\dots ,{\\frac {\\partial f}{\\partial x_{n}}}\\right]$$\n",
        "\n",
        "Niech $\\mathcal{L}: \\mathbb{R}^D \\rightarrow \\mathbb{R}$ będzie pewną funkcją straty określoną dla sieci neuronowej o $D$ parametrach (wagach).\n",
        "Celem treningu sieci neuronowej jest znalezienie zestawu parametrów $\\mathbf{\\hat{}} \\in \\mathbb{R}^D$ minimalizującego wartośc funkcji straty:\n",
        "$$\\mathbf{\\hat{w}} = \\arg \\min_{\\textbf{w}} \\mathcal{L} \\left( \\textbf{w} \\right)$$\n",
        "W metodzie **spadku wzdłuż gradientu** zaczynamy od losowo zainicjalizowanych parametrów (wag) sieci $\\textbf{w}_0$ a następnie iteracyjnie aktualizujemy parametry sieci w kierunku przeciwnym do wartości gradientu:\n",
        "$$\n",
        "\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\eta \\nabla \\mathcal{L} \\left( \\mathbf{w}_t \\right)\n",
        "$$.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "G1OdCLSqaaj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aby wyznaczyć **gradient funkcji straty względem parametrów sieci**, PyTorch posiada wbudowany mechanizm różniczkowania o nazwie `torch.autograd`. Umożliwia on automatyczne obliczanie gradientu dla dowolnego grafu obliczeniowego.\n",
        "\n",
        "Obiekty typu Tensor posiadają logiczną flagę `requires_grad`.\n",
        "Domyślnie flaga `requires_grad` jest ustawiana na `False`.\n",
        "Po jej włączeniu PyTorch będzie automatycznie budował grafy dla wszystkich obliczeń wykonanych z wykorzystaniem tego tensora aby umożliwić automatyczne wyznaczanie gradientu.\n",
        "Jeśli jeden z argumentów operacji na tensorach ma ustawioną flagę `requires_grad`, wynik również będzie miał ustawioną tę flagę."
      ],
      "metadata": {
        "id": "bWXwwnMI2Eqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([ 1., 2. ])\n",
        "y = torch.tensor([ 4., 5. ])\n",
        "z = torch.tensor([ 7., 3. ])\n",
        "print(f\"{x.requires_grad=}\")\n",
        "print(f\"{y.requires_grad=}\")\n",
        "print(f\"{(x + y).requires_grad=}\\n\")\n",
        "\n",
        "x.requires_grad = True\n",
        "print(f\"{x.requires_grad=}\")\n",
        "print(f\"{y.requires_grad=}\")\n",
        "print(f\"{(x + z).requires_grad=}\")"
      ],
      "metadata": {
        "id": "Hn3ddlgJ2EyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Przykład 1\n",
        "Rozważmy funkcję:\n",
        "$$\n",
        "f(t, u) = \\sum_i t_i^2 + \\sum_j \\log u_j \\, ,\n",
        "$$\n",
        "gdzie $t \\in \\mathbb{R}^n, u \\in \\mathbb{R}^m$.\n",
        "\n",
        "Zaimplementujmy wyznaczenie wartości tej funkcji w PyTorch dla przykładowych parametrów $t=[1,2,4]$ i $u=[10,20]$. Aby było możliwe wyznaczenie wartości gradientu $f$ dla tych parametrów włączymy flagę `requires_grad`."
      ],
      "metadata": {
        "id": "IyTEmqsmeb-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.tensor([1., 2., 4.])\n",
        "t.requires_grad = True\n",
        "u = torch.tensor([10., 20.])\n",
        "u.requires_grad = True\n",
        "\n",
        "f = t.pow(2).sum() + u.log().sum()\n",
        "print(f\"{f=}\")\n",
        "print(f\"{f.requires_grad=}\")"
      ],
      "metadata": {
        "id": "FbwsJHuceF7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wizualizacja grafu obliczeń dla tensora `f`."
      ],
      "metadata": {
        "id": "_KR7CIZ_eKxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "from IPython.display import Image, display\n",
        "\n",
        "dot = make_dot(f, params={\"t\": t, \"u\": u, \"f\": f})\n",
        "dot.render(\"computational_graph\", format=\"png\")\n",
        "display(Image(filename=\"computational_graph.png\"))"
      ],
      "metadata": {
        "id": "fJVtqJR0cDpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metoda `f.backward()` wykonuje przejście w tył przez graf obliczeń w celu wyznaczenia gradientów względem parametrów (tensorów z włączoną flagą `requires_grad`) wykorzystanych do wyznaczenia wartości `f`. W naszym przypadku są to tensory `t` i `u`.\n",
        "\n",
        "Pochodne cząstkowe funkcji $f$ względem parametrów $t$ i $u$ możemy wyznaczyć analitycznie:\n",
        "$\n",
        "\\frac{\\partial f}{\\partial t_i} = 2 t_i\n",
        "$ i\n",
        "$\n",
        "\\frac{\\partial f}{\\partial u_i} = \\frac{1}{u_i}\n",
        "$.\n",
        "Jak widać wyznaczone analitycznie wartości gradientu są identyczne jak wartości wyznaczone automatyczniez wykorzystaniem grafu obliczeń.\n"
      ],
      "metadata": {
        "id": "KqiMRy1E5s8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nPrzed wykonaniem f.backward()\")\n",
        "print(f\"{t.grad=}\")\n",
        "print(f\"{u.grad=}\")\n",
        "\n",
        "f.backward()\n",
        "print(f\"\\nPo wykonaniu f.backward()\")\n",
        "print(f\"{t.grad=}\")\n",
        "print(f\"{u.grad=}\")"
      ],
      "metadata": {
        "id": "STX6P4VF5tSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Przykład 2\n",
        "Rozważmy najprostszą sieć neuronową złożoną z jednej warstwy liniowej z wejściem `x` $\\in \\mathbb{R}^{5}$ oraz pewną funkcję straty `loss`.\n",
        "Warstwa liniowa jest zaimplementowana w PyTorch jako klasa `nn.Linear` ([link](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#linear)).\n",
        "\n",
        "Do celów poglądowych zasymulujemy działanie warstwy linowej korzystając z macierzy wag `w` $\\in \\mathbb{R}^{5 \\times 3}$ i wektora obciążenia (*bias*) `b` $\\in \\mathbb{R}^{3}$.\n",
        "`w` i `b` są **parametrami**, które chcemy optymalizować w procesie uczenia sieci. W tym celu musimy wyznaczyć gradient funkcji straty względem tych parametrów. Aby umożliwić automatyczne wyznaczanie gradientu dla tych parametrów, ustawiamy dla nich atrybut `requires_grad` na `true`.\n",
        "Zauważmy, że tensory utworzone w oparciu o choćby jeden tensor z atrybutem `requires_grad`, w tym przypadku `z` i `loss`, również będą miały aktywne wyznaczanie gradientu.\n",
        "\n",
        "**Uwaga**: Jesli wykorzystujemy moduły sieci dziedziczące z klasy `nn.Module` (np. `nn.Linear`, `nn.Conv2D`) tensory parametrów (wag) mają atrybut `requires_grad` domyślnie ustawiony na `true`."
      ],
      "metadata": {
        "id": "n1_mFIi8WfwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "x = torch.ones(5)           # Tensor wejściowy\n",
        "y = torch.zeros(3)          # Oczekiwane wyjście\n",
        "\n",
        "# Parametry warstwy liniowej: macierz wag i wektor obciążenia (bias)\n",
        "w = torch.randn((5, 3), requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "\n",
        "z = x @ w + b\n",
        "\n",
        "# Wyznaczmy wartość funkcji straty jako funkcję entropii krzyżowej między\n",
        "# wyjściem z sieci a oczekiwanym wyjściem\n",
        "loss = nn.functional.binary_cross_entropy_with_logits(z, y)"
      ],
      "metadata": {
        "id": "B8iumeAUlGIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{z=}\")\n",
        "print(f\"{z.requires_grad=}\")\n",
        "print(f\"{loss=}\")\n",
        "print(f\"{loss.requires_grad=}\")"
      ],
      "metadata": {
        "id": "UOqlSecfOIcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Wizualizacja grafu obliczeń\n",
        "\n"
      ],
      "metadata": {
        "id": "DOfcSDtFlW0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchviz import make_dot\n",
        "from IPython.display import Image, display\n",
        "\n",
        "dot = make_dot(loss, params={\"w\": w, \"b\": b, \"loss\": loss})\n",
        "dot.render(\"computational_graph\", format=\"png\")\n",
        "display(Image(filename=\"computational_graph.png\"))"
      ],
      "metadata": {
        "id": "Ub1I9gpxJdVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funkcje i operatory które stosujemy do tensorów w celu skonstruowania grafu obliczeń, takie jak operatory dodawania i mnożenia dwóch tensorów czy funkcja `binary_cross_entropy_with_logits`, są zaimplementowne jako klasy dziedziczące z klasy `Function`. Mają zdefiniowaną metodę `forward` wyznaczającą wartość operatora/funkcji oraz metodę `backward` wyznaczającą jej pochodną w kroku propagacji wstecznej.\n",
        "Odwołanie do funkcji propagacji wstecznej tensora jest przechowywane w atrybucie `grad_fn`.\n",
        "Więcej informacji na temat klasy Function można znaleźć tutaj: [link](https://www.google.com/url?q=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fautograd.html%23function)."
      ],
      "metadata": {
        "id": "IRX4TzmdY_eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ],
      "metadata": {
        "id": "8ZwEJZN0OR_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Obliczanie gradientu\n",
        "\n",
        "Aby wykonać jeden krok optymalizacji wag sieci musimy wyznaczyć gradient funkcji straty względem parametrów sieci, czyli $\\frac{\\partial loss}{\\partial w}$ i\n",
        "$\\frac{\\partial loss}{\\partial b}$ dla ustalonych wartości `x` i\n",
        "`y`.\n",
        "Aby wyznaczyć te pochodne cząstkowe wywołujemy funkcję `loss.backward()`. Wyznaczone pochodne zostaną zapisane w atrybutach `w.grad` and `b.grad`."
      ],
      "metadata": {
        "id": "hCSFfyyActgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print(f\"{w.grad=}\")\n",
        "print(f\"{b.grad=}\")"
      ],
      "metadata": {
        "id": "xIjQdIBjOavt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zasymulujemy teraz jeden krok optymalizacji parametrów sieci metodą spadku wzdłuż gradientu.\n",
        "Gradient funkcji straty `loss` względem parametrów (macierz wag `w` i wektor obciążeń `b`) wyznaczony w poprzednim kroku określa kierunek maksymalnego wzrostu wartości funkcji straty.\n",
        "Ponieważ chcemy minimalizować wartość funkcji straty, musimy zmieniać parametry w kierunku przeciwnym do gradientu.\n",
        "\n",
        "Graf obliczeń powinien być budowany tylko dla obliczeń prowadzących do wyznaczenia wartości funkcji straty.\n",
        "Przy samej aktualizacji parametrów sieci z wykorzystaniem wyznaczonych gradientów powinniśmy wyłączyć budowanie grafu obliczeń korzystając z menadżera kontekstu `torch.no_grad():`.\n"
      ],
      "metadata": {
        "id": "9lNba56oOHBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Początkowa wartość funkcji straty: {loss:.5f}\")\n",
        "\n",
        "learning_rate = .5\n",
        "with torch.no_grad():\n",
        "    w -= learning_rate * w.grad\n",
        "    b -= learning_rate * b.grad\n",
        "\n",
        "z = x @ w + b\n",
        "loss2 = nn.functional.cross_entropy(z, y)\n",
        "\n",
        "print(f\"Wartość funkcji straty po jednym kroku optymalizacji parametrów: {loss2:.5f}\")"
      ],
      "metadata": {
        "id": "oXJ3jjCBLy0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uwaga:** Ze względów wydajnościowych wyznaczone pochodne są zapisywane tylko dla liści grafu obliczeń z ustawionym atrybutem `requires_grad`. Dla wewnętrznych węzłów w grafie obliczeń (w naszym przypadku `z` i `loss`) atrybut `grad` nie jest zapisywany."
      ],
      "metadata": {
        "id": "54o-6J8CeX_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{z.grad=}\")\n",
        "print(f\"{loss.grad=}\")"
      ],
      "metadata": {
        "id": "lkHR2JJnffNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zapisywanie wartości wyznaczonego gradientu dla wewnętrznych węzłów w grafie obliczeń można włączyć wywołując dla nich metodę `retain_grad()`."
      ],
      "metadata": {
        "id": "1LgMfut1EIIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parametry warstwy liniowej: macierz wag i wektor obciążenia (bias)\n",
        "w = torch.randn((5, 3), requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "\n",
        "z = x @ w + b\n",
        "z.retain_grad()\n",
        "\n",
        "# Wyznaczmy wartość funkcji straty jako funkcję entropii krzyżowej między\n",
        "# wyjściem z sieci a oczekiwanym wyjście\n",
        "loss = nn.functional.binary_cross_entropy_with_logits(z, y)\n",
        "loss.retain_grad()\n",
        "\n",
        "loss.backward()\n",
        "print(f\"{z.grad=}\")\n",
        "print(f\"{loss.grad=}\")\n",
        "print(f\"{w.grad=}\")\n",
        "print(f\"{b.grad=}\")"
      ],
      "metadata": {
        "id": "vTn8jbpvEHXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Przykład 3\n",
        "Funkcja\n",
        "$$\\mathcal{L}=\\sqrt{sin \\left( x \\cdot y + z \\right) + 4}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "oyxNz42ZoIeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.)\n",
        "x.requires_grad = True\n",
        "y = torch.tensor(-1.)\n",
        "y.requires_grad = True\n",
        "z = torch.tensor(2.)\n",
        "z.requires_grad = True\n",
        "\n",
        "L = torch.sqrt(torch.sin(x * y + z) + 4)\n",
        "\n",
        "print(f\"{L=}\")\n",
        "print(f\"{L.requires_grad=}\")"
      ],
      "metadata": {
        "id": "ScLX5tUXoTz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L.backward()\n",
        "print(f\"{x.grad=}\")\n",
        "print(f\"{y.grad=}\")\n",
        "print(f\"{z.grad=}\")"
      ],
      "metadata": {
        "id": "MnUx4Qpzo7Zr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot = make_dot(L, params={\"x\": x, \"y\": y, \"z\": z, \"L\": L})\n",
        "dot.render(\"computational_graph\", format=\"png\")\n",
        "display(Image(filename=\"computational_graph.png\"))"
      ],
      "metadata": {
        "id": "HxOtyVeYtSB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Wyłączenie śledzenia historii obliczeń"
      ],
      "metadata": {
        "id": "VMW1CEZGeYwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Historia obliczeń wykonywanych z wykorzystaniem tensorów z atrybutem `requires_grad=True` jest śledzona i automatycznie budowany jest graf obliczeń.  Tensory będące wynkiem takich obliczeń również mają ustawiony atrybut `requires_grad` i historia obliczeń z ich wykonaniem jest dalej śledzona.\n",
        "\n",
        "W niektórych przypadkach nie jest to konieczne. Jeśli chcemy wykorzystać wcześniej wytrenowany model, i nie zamierzamy optymalizować jego parametrów, możemy wyłączyć śledzenie obliczeń korzystając z menadżera kontekstu `torch.no_grad()`.\n",
        "\n",
        "Alternatywnie możemy ustawić atrybut `requires_grad` wszystkich lub wybranych parametrów modelu na `False`. W ten sposób można zamrozić część parametrów modelu i optymalizować tylko pozostałe parametry."
      ],
      "metadata": {
        "id": "qTD5ZYyCf6Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(5)\n",
        "\n",
        "z = x @ w + b\n",
        "print(f\"{z.requires_grad=}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = x @ w + b\n",
        "\n",
        "print(f\"{z.requires_grad=}\")"
      ],
      "metadata": {
        "id": "HBnJjUFyPyEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternatywnie możemy użyć metodę `detach()` aby odłączyć graf obliczeń od istniejącego tensora. Tak uzyskany tensor będzie miał wyłączony atrybut `requires_grad`. Historia obliczeń z jego udziałem nie będzie śledzona i nie będzie możliwe wyznaczenie dla niego gradientu."
      ],
      "metadata": {
        "id": "2xEhlRyYJYrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = x @ w + b\n",
        "print(f\"{z.requires_grad=}\")\n",
        "y = z * 3 + 2\n",
        "print(f\"{y.requires_grad=}\")\n",
        "print()\n",
        "\n",
        "z_det = z.detach()\n",
        "print(f\"{z_det.requires_grad=}\")\n",
        "y_det = z_det * 3 + 2\n",
        "print(f\"{y_det.requires_grad=}\")"
      ],
      "metadata": {
        "id": "amfVtuKyf-3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8xwUHzfAXf9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Przykład: Regresja logistyczna\n",
        "Rozważny problem dwuklasowej regresji logistycznej:\n",
        "$$P(Y=1|X=x) = \\sigma(W \\cdot x + b) \\, ,$$\n",
        "gdzie $\\sigma(x)$ jest **funkcją sigmoidalną** daną wzorem: $$\\sigma(x) = \\frac{1}{1+\\exp(-x)} \\, ,$$\n",
        "a zmienna zależna $Y$ przyjmuje wartości ze zbioru $\\left\\{ 0, 1 \\right\\}$.\n",
        "\n",
        "Funkcja straty binarnej entropii krzyżowej między prawdziwym a estymowanym prawdopodobieństwem dana jest wzorem:\n",
        "$$\n",
        "\\mathcal{L} = y_i \\log \\sigma \\left(z_i\\right) + (1 - y_i) \\log\n",
        "\\left( 1 - \\sigma \\left(z_i\\right)  \\right) \\, ,\n",
        "$$\n",
        "gdzie\n",
        "$$\n",
        "z_i = W \\cdot x_i + b\n",
        "$$\n",
        "\n",
        "Problem ten nie ma rozwiązania analitycznego. Wartość parametrów (macierz wag $W$ i wektor obciążenia $b$) możemy znaleźć metodą spadku wzdłuż gradientu."
      ],
      "metadata": {
        "id": "Kzr0qb_Yi8uG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wygenerujmy syntetyczny zbiór danych korzystając z funkcji `make_classification` z biblkioteki scikit-learn ([link](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html))."
      ],
      "metadata": {
        "id": "hdhZTTm_o1z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Wygeneruj syntetyczny zbiór danych\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, n_informative=5, random_state=39)\n",
        "\n",
        "# Podziel na dane treningowe i testowe\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(f\"Liczba elementów w zbiorze danych: {X.shape[0]}\")\n",
        "print(f\"Liczba cech: {X.shape[1]}\")\n",
        "print(f\"Liczba klas: {len(np.unique(y))}\")\n",
        "\n",
        "# Podziel na częśc treningową (80%) i testową (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ],
      "metadata": {
        "id": "io8tSKtoyzAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[:6])\n",
        "print(y_train[:6])"
      ],
      "metadata": {
        "id": "Wes9uC1DzIEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wizualizacja dwóch wybranych cech. Możemy zauważyć, że dla dwóch losowo wybranych cech elementy z różnych klas nie są liniowo separowalne."
      ],
      "metadata": {
        "id": "dYZ7hyTo-nD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Wybierz dwie losowe cechy\n",
        "f1 = random.randint(0, X_train.shape[1]-1)\n",
        "f2 = random.randint(0, X_train.shape[1]-1)\n",
        "\n",
        "plt.scatter(X_train[y_train==0, f1], X_train[y_train==0, f2], color='blue', label='Klasa 0', alpha=0.7)\n",
        "plt.scatter(X_train[y_train==1, f1], X_train[y_train==1, f2], color='red', label='Klasa 1', alpha=0.7)\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.xlabel(f\"Wartość cechy {f1}\")\n",
        "plt.ylabel(f\"Wartość cechy {f2}\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "i0eL9Jz7AEDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do wyznaczenia wartości wyrażenia $z_i = W  x_i + b$ wykorzystamy **warstwę liniową** zaimplementowaną w bibliotece PyTorch jako moduł sieci `nn.Linear` ([link](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)).\n",
        "Utworzymy warstwę liniową o wymiarze wejścia $=10$ i wymiarze wyjścia $=1$ (czyli na wejściu będzie wektor $x_i \\in \\mathbb{R}^{10}$ a na wyjściu skalar $z_i \\in \\mathbb{R}$).\n",
        "\n",
        "Zauważmy, że utworzona warstwa liniowa (obiekt `linear`) ma dwa parametry: macierz wag `linear.weight` i wektor obciążenia `linear.bias`. Każdy z nich jest tensorem o losowo zainicjalizowanych wartościach z włączonym śledzeniem historii obliczeń (`requires_grad=True`)."
      ],
      "metadata": {
        "id": "huWMpDjGDmjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "n_features = X_train.shape[1]\n",
        "linear = nn.Linear(n_features, 1)\n",
        "\n",
        "print(f\"{linear=}\\n\")\n",
        "print(f\"{linear.weight=}\\n\")\n",
        "print(f\"{linear.bias=}\")"
      ],
      "metadata": {
        "id": "JdWkxGLoy3Bx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sprawdźmy działanie warstwy liniowej.\n",
        "Po przetworzeniu wektora cech $x \\in \\mathbb{R}^{10}$ w rezultacie otrzymamy pojedyczną wartość rzeczywistą (logit).\n",
        "Warstwa liniowa pozwoli również przetworzyć cały wsad wektorów cech $X \\in \\mathbb{R}^{N \\times 10}$ i w rezultacie otrzymamy wektor $N$ logitów.\n",
        "Zauważmy, że tensor `z` zawierający wynikowe wartości ma również włączone śledzenie obliczeń (`requires_grad=True`).\n"
      ],
      "metadata": {
        "id": "jpfz6yBUFFcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zamień dane (jako tablice ndarray) na tensory\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "elem_ndx = 10\n",
        "z = linear(X_train_tensor[elem_ndx])\n",
        "print(f\"Rozmiar wejścia: {X_train_tensor[elem_ndx].shape}\")\n",
        "print(f\"Rozmiar wyjścia: {z.shape}\")\n",
        "print(f\"Wyjście: {z}\")\n",
        "print(f\"{z.requires_grad=}\")\n",
        "print()\n",
        "\n",
        "z = linear(X_train_tensor)\n",
        "print(f\"Rozmiar wejścia: {X_train_tensor.shape}\")\n",
        "print(f\"Rozmiar wyjścia: {z.shape}\")\n",
        "print(f\"Wyjście (pierwsze 5 elementów): {z[:5]}\")\n",
        "print(f\"{z.requires_grad=}\")"
      ],
      "metadata": {
        "id": "vVwTzgt2Joqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optymalizacja parametrów (wag) warstwy liniowej metodą spadku wzdłuż gradientu.\n",
        "Do obliczania wartości straty wykorzystamy funkcję `BCEWithLogitsLoss`, łączącą w jednej klasie funkcję Sigmoid i funkcję straty binarnej entropii krzyżowej (patrz: [link](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss)).\n",
        "Pozwala to na bardziej stabilne numerycznie obliczenie niż zastosowanie najpierw funkcji Sigmoid a następnie wyznaczenie straty binarnej entropii krzyżowej.\n",
        "\n",
        "W literaturze poświęconej głębokiemu uczeniu nieznormalizowane wyjścia z modułu sieci, w naszym przypadku z warstwy liniowej, mogące przyjmować wartości rzeczywiste z zakresu $\\left( -\\infty, \\infty \\right)$ zwane są **logitami**.\n",
        "\n",
        "**Uwaga:** Poniższy kod ma charakter poglądowy. W praktyce optymalizację parametrów (wag) sieci neuronowej wykonujemy korzystając z zaimplementowanego w PyTorch optymalizatora (np. SGD lub Adam)."
      ],
      "metadata": {
        "id": "HBMUSHaR5HAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "eta= 0.1\n",
        "n_epochs = 200\n",
        "for k in range(n_epochs):\n",
        "    # Wyzeruj wartości gradientu przed kolejnym przejściem w tył\n",
        "    # Patrz: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n",
        "    linear.zero_grad()\n",
        "\n",
        "    logits = linear(X_train_tensor)\n",
        "    # logits ma rozmiar (N, 1)\n",
        "    logits = logits.squeeze(1)\n",
        "    # logits ma rozmiar (N,)\n",
        "\n",
        "    # Wyznacz wartość funkcji straty\n",
        "    loss = criterion(logits, y_train_tensor)\n",
        "\n",
        "    # Przejście w tył- wyznaczenie gradientu funkcji straty względem parametrów (wag) modelu\n",
        "    loss.backward()\n",
        "\n",
        "    # Krok optymalizacji parametrów sieci - zmiana w kierunku przeciwnym do wartości gradientu\n",
        "    with torch.no_grad():\n",
        "        linear.weight -= eta * linear.weight.grad\n",
        "        linear.bias -= eta * linear.bias.grad\n",
        "\n",
        "    # Dokładność klasyfikacji na zbiorze treningowym\n",
        "    y_pred_labels = (logits.detach() >= 0.5).float()\n",
        "    train_accuracy = (y_pred_labels == y_train_tensor).sum().item() / y_train_tensor.size(0)\n",
        "\n",
        "    # Dokładność klasyfikacji na zbiorze testowym\n",
        "    with torch.no_grad():\n",
        "        logits = linear(X_test_tensor)\n",
        "        logits = logits.squeeze(1)\n",
        "        y_pred_labels = (logits >= 0.5).float()\n",
        "        test_accuracy = (y_pred_labels == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
        "\n",
        "    if k % 20 == 0:\n",
        "        print(f\"Epoch: {k}   Wartość funkcji straty: {loss.item():.5f}   Dokładność (train): {train_accuracy:.4f}   Dokładność (test): {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "6WDIz_TPAREJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dla porównania sprawdzimy jaką skuteczność osiąga klasyfikator regresji logistycznej z biblioteki scikit-learn."
      ],
      "metadata": {
        "id": "w7KJ_ZKv4O97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Evaluate the model\n",
        "y_train_pred = model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "y_test_pred = model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Dokładność (train): {train_accuracy:.4f}   Dokładność (test): {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "9k3sCEZjNOjj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}